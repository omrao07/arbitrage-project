{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6085e690",
   "metadata": {},
   "source": [
    "\n",
    "# 🗞️ Sentiment Models — Research & Backtest\n",
    "\n",
    "_Date generated: 2025-09-03_\n",
    "\n",
    "End-to-end notebook to prototype **news / transcript sentiment** and test if it predicts future returns.\n",
    "\n",
    "**What you get**\n",
    "- Data loaders (CSV) with **synthetic fallback**\n",
    "- Simple **lexicon** rule-based sentiment\n",
    "- Lightweight **Multinomial Naive Bayes** text model (no external deps)\n",
    "- Optional **transformer** inference (if `transformers` is available locally)\n",
    "- Aggregation to **daily ticker sentiment**\n",
    "- Predictive tests: **IC**, **long/short bucket**, and **event study**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7ac3b0",
   "metadata": {},
   "source": [
    "## 0) Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a118978c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Expected CSVs (optional). If absent, the notebook synthesizes examples.\n",
    "# news.csv columns: date, ticker, title, body\n",
    "# transcripts.csv columns: date, ticker, speaker, text\n",
    "# returns.csv columns: date, TICK1, TICK2, ... (daily returns in decimal)\n",
    "PATH_NEWS = \"data/news.csv\"\n",
    "PATH_TRANS = \"data/transcripts.csv\"\n",
    "PATH_RETURNS = \"data/returns.csv\"\n",
    "\n",
    "# Controls\n",
    "TOP_K_WORDS = 5000          # vocab size for NB model\n",
    "MIN_DOCS_PER_TICKER = 10    # filter sparse tickers for tests\n",
    "EVENT_WINDOW = 5            # ± days around events for event study\n",
    "IC_HORIZON = 1              # next-day prediction by default\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64f5204",
   "metadata": {},
   "source": [
    "## 1) Setup & Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bc652f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, re, math, random, warnings\n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.options.display.float_format = \"{:,.4f}\".format\n",
    "\n",
    "random.seed(7); np.random.seed(7)\n",
    "\n",
    "# Basic tokenizer\n",
    "_word_re = re.compile(r\"[A-Za-z']+\")\n",
    "def tokenize(text: str) -> List[str]:\n",
    "    if not isinstance(text, str): return []\n",
    "    return [w.lower() for w in _word_re.findall(text)]\n",
    "\n",
    "def train_test_split(df, test_frac=0.2, seed=7):\n",
    "    idx = np.arange(len(df))\n",
    "    rng = np.random.default_rng(seed)\n",
    "    rng.shuffle(idx)\n",
    "    split = int(len(idx) * (1 - test_frac))\n",
    "    return df.iloc[idx[:split]], df.iloc[idx[split:]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5ab6e7",
   "metadata": {},
   "source": [
    "## 2) Load Data (CSV or synthetic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f634dadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_news(path=PATH_NEWS) -> pd.DataFrame:\n",
    "    if os.path.exists(path):\n",
    "        df = pd.read_csv(path, parse_dates=[\"date\"])\n",
    "        return df\n",
    "    # Synthetic news\n",
    "    dates = pd.bdate_range(\"2024-01-01\", periods=220)\n",
    "    tickers = [\"AAPL\",\"MSFT\",\"AMZN\",\"GOOG\",\"TSLA\",\"META\",\"NFLX\",\"NVDA\"]\n",
    "    pos_phr = [\"beats expectations\", \"strong demand\", \"surge\", \"upgrade\", \"record revenue\", \"partnership\"]\n",
    "    neg_phr = [\"misses estimates\", \"probe\", \"downgrade\", \"weak guidance\", \"lawsuit\", \"recall\"]\n",
    "    rows = []\n",
    "    rng = np.random.default_rng(1)\n",
    "    for d in dates:\n",
    "        for t in rng.choice(tickers, size=rng.integers(1, 4), replace=False):\n",
    "            polarity = rng.choice([1, -1], p=[0.55, 0.45])\n",
    "            if polarity == 1:\n",
    "                title = f\"{t} {rng.choice(pos_phr)}\"\n",
    "                body = f\"{t} reports {rng.choice(['robust','solid','accelerating'])} growth and {rng.choice(pos_phr)}.\"\n",
    "            else:\n",
    "                title = f\"{t} {rng.choice(neg_phr)}\"\n",
    "                body = f\"{t} faces {rng.choice(['headwinds','concerns','slowing'])} and {rng.choice(neg_phr)}.\"\n",
    "            rows.append({\"date\": d, \"ticker\": t, \"title\": title, \"body\": body, \"label\": 1 if polarity==1 else 0})\n",
    "    df = pd.DataFrame(rows)\n",
    "    return df\n",
    "\n",
    "def load_transcripts(path=PATH_TRANS) -> pd.DataFrame:\n",
    "    if os.path.exists(path):\n",
    "        return pd.read_csv(path, parse_dates=[\"date\"])\n",
    "    # Synthetic transcripts: neutral to mildly polarized\n",
    "    dates = pd.bdate_range(\"2024-01-01\", periods=60, freq='B')\n",
    "    tickers = [\"AAPL\",\"MSFT\",\"AMZN\",\"GOOG\",\"TSLA\",\"META\",\"NFLX\",\"NVDA\"]\n",
    "    speakers = [\"CEO\",\"CFO\",\"Analyst\"]\n",
    "    rows = []\n",
    "    rng = np.random.default_rng(2)\n",
    "    phrases_pos = [\"confident\", \"pipeline strong\", \"margin expansion\", \"tailwinds\"]\n",
    "    phrases_neg = [\"cautious\", \"pressure\", \"uncertain\", \"headwinds\"]\n",
    "    for d in dates:\n",
    "        for t in rng.choice(tickers, size=rng.integers(1,4), replace=False):\n",
    "            for _ in range(rng.integers(1,3)):\n",
    "                pol = rng.choice([1,0], p=[0.5, 0.5])\n",
    "                if pol:\n",
    "                    text = f\"We are {rng.choice(phrases_pos)} and see {rng.choice(phrases_pos)} ahead.\"\n",
    "                else:\n",
    "                    text = f\"We remain {rng.choice(phrases_neg)} due to {rng.choice(phrases_neg)}.\"\n",
    "                rows.append({\"date\": d, \"ticker\": t, \"speaker\": rng.choice(speakers), \"text\": text})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def load_returns(path=PATH_RETURNS, n_assets=8, days=260) -> pd.DataFrame:\n",
    "    if os.path.exists(path):\n",
    "        return pd.read_csv(path, parse_dates=[\"date\"]).set_index(\"date\").sort_index()\n",
    "    rng = np.random.default_rng(3)\n",
    "    dates = pd.bdate_range(\"2024-01-01\", periods=days)\n",
    "    cols = [\"AAPL\",\"MSFT\",\"AMZN\",\"GOOG\",\"TSLA\",\"META\",\"NFLX\",\"NVDA\"]\n",
    "    mu = rng.normal(0.0005, 0.0002, size=len(cols))\n",
    "    sd = rng.uniform(0.01, 0.02, size=len(cols))\n",
    "    data = [rng.normal(mu[i], sd[i], size=len(dates)) for i in range(len(cols))]\n",
    "    return pd.DataFrame(np.array(data).T, index=dates, columns=cols)\n",
    "\n",
    "news = load_news()\n",
    "trans = load_transcripts()\n",
    "rets = load_returns()\n",
    "\n",
    "news.head(), trans.head(), rets.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c17e9e6",
   "metadata": {},
   "source": [
    "## 3) Lexicon Sentiment (rule-based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d600a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "POS = set(['beat','beats','strong','surge','upgrade','record','growth','bullish','confident','tailwinds','expansion'])\n",
    "NEG = set(['miss','misses','probe','downgrade','weak','lawsuit','recall','headwinds','uncertain','pressure','cautious'])\n",
    "\n",
    "def lexicon_score(text: str) -> float:\n",
    "    toks = tokenize(text)\n",
    "    pos = sum(1 for w in toks if w in POS)\n",
    "    neg = sum(1 for w in toks if w in NEG)\n",
    "    if pos==0 and neg==0: return 0.0\n",
    "    return (pos - neg) / max(1, pos + neg)\n",
    "\n",
    "def apply_lexicon(df: pd.DataFrame, text_cols: List[str]) -> pd.Series:\n",
    "    scores = []\n",
    "    for _, row in df.iterrows():\n",
    "        txt = \" \".join(str(row[c]) for c in text_cols if c in df.columns)\n",
    "        scores.append(lexicon_score(txt))\n",
    "    return pd.Series(scores, index=df.index)\n",
    "\n",
    "news[\"lexicon_score\"] = apply_lexicon(news, [\"title\", \"body\"])\n",
    "trans[\"lexicon_score\"] = apply_lexicon(trans, [\"text\"])\n",
    "news[[\"date\",\"ticker\",\"title\",\"lexicon_score\"]].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909f3c4c",
   "metadata": {},
   "source": [
    "## 4) Lightweight Text Model — Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27b8d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MiniCountVectorizer:\n",
    "    def __init__(self, max_features=5000):\n",
    "        self.max_features = max_features\n",
    "        self.vocab_ = {}\n",
    "        self.id2tok_ = []\n",
    "\n",
    "    def fit(self, texts: List[str]):\n",
    "        counts = Counter()\n",
    "        for t in texts:\n",
    "            counts.update(tokenize(t))\n",
    "        most = counts.most_common(self.max_features)\n",
    "        self.id2tok_ = [w for w,_ in most]\n",
    "        self.vocab_ = {w:i for i,w in enumerate(self.id2tok_)}\n",
    "        return self\n",
    "\n",
    "    def transform(self, texts: List[str]) -> np.ndarray:\n",
    "        X = np.zeros((len(texts), len(self.vocab_)), dtype=np.float32)\n",
    "        for i,t in enumerate(texts):\n",
    "            for w in tokenize(t):\n",
    "                j = self.vocab_.get(w, -1)\n",
    "                if j>=0:\n",
    "                    X[i,j]+=1.0\n",
    "        return X\n",
    "\n",
    "class MultinomialNB:\n",
    "    def __init__(self, alpha=1.0):\n",
    "        self.alpha = alpha\n",
    "        self.class_log_prior_ = None\n",
    "        self.feature_log_prob_ = None\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        n_classes = int(y.max()) + 1\n",
    "        self.class_log_prior_ = np.log(np.bincount(y) / len(y))\n",
    "        # Laplace smoothing\n",
    "        smoothed_fc = []\n",
    "        for c in range(n_classes):\n",
    "            Xc = X[y==c]\n",
    "            fc = Xc.sum(axis=0) + self.alpha\n",
    "            smoothed_fc.append(fc / fc.sum())\n",
    "        self.feature_log_prob_ = np.log(np.vstack(smoothed_fc))\n",
    "        return self\n",
    "\n",
    "    def predict_log_proba(self, X: np.ndarray) -> np.ndarray:\n",
    "        # log P(c) + sum x_i * log P(w_i|c)\n",
    "        return self.class_log_prior_[None,:] + X @ self.feature_log_prob_.T # type: ignore\n",
    "\n",
    "    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
    "        logp = self.predict_log_proba(X)\n",
    "        # softmax\n",
    "        logp -= logp.max(axis=1, keepdims=True)\n",
    "        p = np.exp(logp)\n",
    "        p /= p.sum(axis=1, keepdims=True)\n",
    "        return p\n",
    "\n",
    "# Use synthetic labels in news if present; otherwise weak labels via lexicon\n",
    "if \"label\" not in news.columns:\n",
    "    news[\"label\"] = (news[\"lexicon_score\"] > 0).astype(int)\n",
    "\n",
    "train_df, test_df = train_test_split(news, test_frac=0.2, seed=7)\n",
    "vec = MiniCountVectorizer(max_features=TOP_K_WORDS).fit((train_df[\"title\"] + \" \" + train_df[\"body\"]).tolist())\n",
    "Xtr = vec.transform((train_df[\"title\"] + \" \" + train_df[\"body\"]).tolist())\n",
    "Xte = vec.transform((test_df[\"title\"] + \" \" + test_df[\"body\"]).tolist())\n",
    "ytr = train_df[\"label\"].astype(int).values\n",
    "yte = test_df[\"label\"].astype(int).values\n",
    "\n",
    "nb = MultinomialNB(alpha=0.5).fit(Xtr, ytr) # type: ignore\n",
    "proba = nb.predict_proba(Xte)[:,1]\n",
    "pred = (proba >= 0.5).astype(int)\n",
    "\n",
    "acc = (pred==yte).mean()\n",
    "from sklearn.metrics import roc_auc_score if False else None # type: ignore\n",
    "# Manual AUC (fallback) to avoid sklearn dep\n",
    "def auc_approx(y_true, y_score, n_bins=100):\n",
    "    df = pd.DataFrame({\"y\": y_true, \"p\": y_score}).sort_values(\"p\")\n",
    "    # trapezoidal ROC approximation\n",
    "    thresholds = np.linspace(0,1,n_bins)\n",
    "    tprs, fprs = [], []\n",
    "    P = (df[\"y\"]==1).sum(); N = (df[\"y\"]==0).sum()\n",
    "    for th in thresholds:\n",
    "        tp = ((df[\"p\"]>=th) & (df[\"y\"]==1)).sum()\n",
    "        fp = ((df[\"p\"]>=th) & (df[\"y\"]==0)).sum()\n",
    "        tprs.append(tp/max(1,P)); fprs.append(fp/max(1,N))\n",
    "    a = 0.0\n",
    "    for i in range(1,len(thresholds)):\n",
    "        a += 0.5*(tprs[i]+tprs[i-1])*(fprs[i]-fprs[i-1])\n",
    "    return 1 - abs(a)\n",
    "\n",
    "auc = auc_approx(yte, proba)\n",
    "acc, auc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d72d918",
   "metadata": {},
   "source": [
    "### Precision by Threshold (approx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0986e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ths = np.linspace(0,1,31)\n",
    "prec, rec = [], []\n",
    "for th in ths:\n",
    "    p = (proba>=th).sum()\n",
    "    tp = ((proba>=th) & (yte==1)).sum()\n",
    "    fn = ((proba<th) & (yte==1)).sum()\n",
    "    precision = (tp/max(1,p))\n",
    "    recall = (tp/max(1,tp+fn))\n",
    "    prec.append(precision); rec.append(recall)\n",
    "\n",
    "plt.figure(figsize=(8,3.2))\n",
    "plt.plot(rec, prec)\n",
    "plt.title(\"Precision vs Recall (threshold sweep)\")\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba76bc7",
   "metadata": {},
   "source": [
    "## 5) Optional Transformer Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69af5c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "HAS_TRANS = False\n",
    "try:\n",
    "    import transformers  # noqa\n",
    "    HAS_TRANS = True\n",
    "except Exception:\n",
    "    HAS_TRANS = False\n",
    "\n",
    "if HAS_TRANS:\n",
    "    from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "    import torch\n",
    "    # NOTE: Requires model to be available locally (no internet). If not, this block will be skipped.\n",
    "    model_name = os.environ.get(\"FINBERT_MODEL_PATH\", \"\")  # e.g., a local dir of ProsusAI/finbert\n",
    "    transformer_ok = False\n",
    "    if model_name and os.path.exists(model_name):\n",
    "        try:\n",
    "            tok = AutoTokenizer.from_pretrained(model_name)\n",
    "            mdl = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "            mdl.eval()\n",
    "            transformer_ok = True\n",
    "        except Exception:\n",
    "            transformer_ok = False\n",
    "\n",
    "    if transformer_ok:\n",
    "        texts = (test_df[\"title\"] + \". \" + test_df[\"body\"]).tolist()[:64]\n",
    "        with torch.no_grad():\n",
    "            enc = tok(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "            out = mdl(**enc)\n",
    "            probs = out.logits.softmax(-1).numpy()  # assume [neg, neutral, pos]\n",
    "            finbert_sent = probs[:, -1] - probs[:, 0]\n",
    "        print(\"Transformer sentiment preview (size):\", finbert_sent.shape)\n",
    "    else:\n",
    "        print(\"Transformers installed but no local model path set; skipping.\")\n",
    "else:\n",
    "    print(\"Transformers not available; skipping.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d759173",
   "metadata": {},
   "source": [
    "## 6) Aggregate Daily Sentiment per Ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880a5287",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use NB probability as signal (fallback to lexicon when unavailable)\n",
    "test_df = test_df.copy()\n",
    "test_df[\"prob_pos\"] = nb.predict_proba(vec.transform((test_df[\"title\"] + \" \" + test_df[\"body\"]).tolist()))[:,1]\n",
    "test_df[\"sentiment\"] = test_df[\"prob_pos\"].where(~test_df[\"prob_pos\"].isna(), test_df[\"lexicon_score\"])\n",
    "\n",
    "daily_sent = test_df.groupby([\"date\",\"ticker\"])[\"sentiment\"].mean().unstack().reindex(rets.index).fillna(0.0)\n",
    "daily_sent.tail()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e13f27",
   "metadata": {},
   "source": [
    "## 7) Predictive Tests — IC & Long/Short Buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ccb970",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "next_ret = rets.shift(-IC_HORIZON)\n",
    "# Cross-sectional IC (Spearman approx via rank corr)\n",
    "def cs_ic(signal_row, return_row):\n",
    "    s = signal_row.dropna(); r = return_row.dropna()\n",
    "    com = s.index.intersection(r.index)\n",
    "    if len(com) < 4: return np.nan\n",
    "    sr = s.loc[com].rank()\n",
    "    rr = r.loc[com].rank()\n",
    "    cov = np.cov(sr, rr)[0,1]\n",
    "    return cov / (sr.std(ddof=1) * rr.std(ddof=1) + 1e-9)\n",
    "\n",
    "ic_series = []\n",
    "for dt in daily_sent.index:\n",
    "    ic_series.append(cs_ic(daily_sent.loc[dt], next_ret.loc[dt]))\n",
    "ic_series = pd.Series(ic_series, index=daily_sent.index)\n",
    "\n",
    "plt.figure(figsize=(10,3))\n",
    "ic_series.plot()\n",
    "plt.axhline(0, linestyle='--')\n",
    "plt.title(\"Daily Information Coefficient (sentiment → next returns)\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "print(\"IC mean =\", float(np.nanmean(ic_series)), \"  IC IR =\", float(np.nanmean(ic_series))/float(np.nanstd(ic_series)+1e-9))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac613f4a",
   "metadata": {},
   "source": [
    "### Long/Short Buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cfd611",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_ls(signal: pd.DataFrame, returns: pd.DataFrame, top_q=0.8, bot_q=0.2):\n",
    "    weights = pd.DataFrame(0.0, index=signal.index, columns=signal.columns)\n",
    "    for dt, row in signal.iterrows():\n",
    "        s = row.dropna()\n",
    "        if s.empty: continue\n",
    "        th_long = s.quantile(top_q); th_short = s.quantile(bot_q)\n",
    "        longs = s.index[s >= th_long]; shorts = s.index[s <= th_short]\n",
    "        if len(longs)>0: weights.loc[dt, longs] =  1.0/len(longs) # type: ignore\n",
    "        if len(shorts)>0: weights.loc[dt, shorts] = -1.0/len(shorts) # type: ignore\n",
    "        # demean to be quasi-market neutral\n",
    "        if weights.loc[dt].abs().sum()>0: # type: ignore\n",
    "            weights.loc[dt] -= weights.loc[dt].mean() # type: ignore\n",
    "            weights.loc[dt] /= weights.loc[dt].abs().sum() # type: ignore\n",
    "    r = (weights.shift(1) * returns).sum(axis=1).fillna(0.0)\n",
    "    return r\n",
    "\n",
    "r_ls = build_ls(daily_sent, rets)\n",
    "equity = (1 + r_ls).cumprod()\n",
    "plt.figure(figsize=(10,3))\n",
    "equity.plot()\n",
    "plt.title(\"Sentiment L/S Equity Curve (no costs)\")\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93fda5c",
   "metadata": {},
   "source": [
    "## 8) Event Study (Top/Bottom Sentiment Days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a202f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def event_study(signal: pd.DataFrame, returns: pd.DataFrame, window=EVENT_WINDOW, top_q=0.95, bot_q=0.05):\n",
    "    # Identify top and bottom quantile events per ticker\n",
    "    events = []\n",
    "    for t in signal.columns:\n",
    "        s = signal[t].dropna()\n",
    "        if len(s) < MIN_DOCS_PER_TICKER: continue\n",
    "        hi = s[s >= s.quantile(top_q)].index\n",
    "        lo = s[s <= s.quantile(bot_q)].index\n",
    "        for dt in hi: events.append((t, dt, 1))\n",
    "        for dt in lo: events.append((t, dt, -1))\n",
    "    # Build windows\n",
    "    rel_days = np.arange(-window, window+1)\n",
    "    mats = []\n",
    "    for t, dt, sign in events:\n",
    "        for k, rd in enumerate(rel_days):\n",
    "            dtk = dt + pd.tseries.offsets.BDay(rd) # type: ignore\n",
    "            if dtk in returns.index:\n",
    "                mats.append({\"rel\": rd, \"ret\": returns.loc[dtk, t], \"sign\": sign})\n",
    "    df = pd.DataFrame(mats)\n",
    "    if df.empty: \n",
    "        return pd.DataFrame(columns=[\"rel\",\"car_pos\",\"car_neg\"])\n",
    "    # Cumulative average returns for pos/neg\n",
    "    pos = df[df[\"sign\"]==1].groupby(\"rel\")[\"ret\"].mean().cumsum()\n",
    "    neg = df[df[\"sign\"]==-1].groupby(\"rel\")[\"ret\"].mean().cumsum()\n",
    "    res = pd.DataFrame({\"car_pos\": pos, \"car_neg\": neg})\n",
    "    res.index.name = \"rel\"\n",
    "    return res\n",
    "\n",
    "ev = event_study(daily_sent, rets, window=EVENT_WINDOW)\n",
    "ev = ev.reindex(range(-EVENT_WINDOW, EVENT_WINDOW+1))\n",
    "ev = ev.fillna(method=\"ffill\").fillna(0.0) # type: ignore\n",
    "\n",
    "plt.figure(figsize=(8,3))\n",
    "plt.plot(ev.index, ev[\"car_pos\"], label=\"Top sentiment CAR\")\n",
    "plt.plot(ev.index, ev[\"car_neg\"], label=\"Bottom sentiment CAR\")\n",
    "plt.title(\"Event Study: Cumulative Average Returns\")\n",
    "plt.legend(); plt.tight_layout(); plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
