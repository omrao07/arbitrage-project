{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca8441f2",
   "metadata": {},
   "source": [
    "\n",
    "# ðŸ¤– RL Execution Agent â€” Almgrenâ€“Chriss Style\n",
    "\n",
    "_Date generated: 2025-09-03_\n",
    "\n",
    "This notebook trains a **reinforcement learning execution agent** to minimize **implementation shortfall** when liquidating an order under market impact, inspired by **Almgrenâ€“Chriss** (temporary & permanent impact).\n",
    "\n",
    "**What you'll get**\n",
    "- A realistic **execution environment** (multi-step, with market volume, spread, volatility, and impact).\n",
    "- Baselines: **TWAP** and **POV** (fixed participation).\n",
    "- **DQN agent** (PyTorch) choosing participation rate each slice.\n",
    "- Learning curve, backtests vs baselines, distribution of costs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df18d8ba",
   "metadata": {},
   "source": [
    "## 1) Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d2da2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math, random, os, sys, time\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, List, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1727e732",
   "metadata": {},
   "source": [
    "## 2) Execution Environment (Almgrenâ€“Chrissâ€‘like)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084852df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class ExecConfig:\n",
    "    T: int = 40                 # time slices\n",
    "    Q0: float = 100_000         # shares to sell\n",
    "    p0: float = 100.0           # arrival mid price\n",
    "    sigma: float = 0.02         # daily vol (fraction)\n",
    "    dt: float = 1/78            # each step ~ 5 minutes (78 slices/day)\n",
    "    kappa: float = 1e-6         # permanent impact per share\n",
    "    eta: float = 5e-5           # temporary impact coefficient\n",
    "    spread_bps: float = 5       # half-spread in bps\n",
    "    vol_mu: float = 2e6         # avg market volume per slice\n",
    "    vol_sd: float = 5e5         # volume std per slice\n",
    "    noise_sd: float = 0.002     # microstructure noise\n",
    "    max_participation: float = 0.2  # hard cap (20% POV per slice)\n",
    "    action_levels: int = 7      # number of discrete participation levels\n",
    "    buy: bool = False           # we default to SELL; set True for buy program\n",
    "\n",
    "class ExecEnv:\n",
    "    def __init__(self, cfg: ExecConfig):\n",
    "        self.cfg = cfg\n",
    "        self.action_space = cfg.action_levels\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.t = 0\n",
    "        self.p = self.cfg.p0\n",
    "        self.q_rem = self.cfg.Q0\n",
    "        self.perm_impact_cum = 0.0\n",
    "        self.last_participation = 0.0\n",
    "        self._gen_paths()\n",
    "        return self._state()\n",
    "\n",
    "    def _gen_paths(self):\n",
    "        T = self.cfg.T\n",
    "        dt = self.cfg.dt\n",
    "        sigma_step = self.cfg.sigma * math.sqrt(dt)\n",
    "        # Exogenous midprice process (without impact)\n",
    "        shocks = np.random.normal(0, sigma_step, size=T)\n",
    "        self.mid_exo = self.cfg.p0 * (1 + np.cumsum(shocks))\n",
    "        # Market volumes per slice\n",
    "        self.market_vol = np.maximum(np.random.normal(self.cfg.vol_mu, self.cfg.vol_sd, size=T), 1e3)\n",
    "        # Spreads per slice (in price units)\n",
    "        self.half_spread = (self.cfg.spread_bps/1e4) * self.cfg.p0\n",
    "\n",
    "    def _state(self):\n",
    "        frac_time = self.t / max(1, self.cfg.T-1)\n",
    "        frac_inv = self.q_rem / max(1, self.cfg.Q0)\n",
    "        avg_vol = self.market_vol[self.t] / (self.cfg.vol_mu + 1e-9)\n",
    "        last_part = self.last_participation\n",
    "        # scaled features\n",
    "        return np.array([frac_time, frac_inv, avg_vol, last_part], dtype=np.float32)\n",
    "\n",
    "    def step(self, action: int):\n",
    "        assert 0 <= action < self.action_space\n",
    "        if self.q_rem <= 0 or self.t >= self.cfg.T:\n",
    "            return self._state(), 0.0, True, {}\n",
    "\n",
    "        # Map discrete action -> participation rate\n",
    "        part = (action / (self.action_space - 1)) * self.cfg.max_participation  # [0, max_participation]\n",
    "        mkt_vol = self.market_vol[self.t]\n",
    "        # shares to execute this slice (can't exceed remaining)\n",
    "        q_exec = min(self.q_rem, part * mkt_vol)\n",
    "\n",
    "        # Impact & price formation\n",
    "        sign = -1 if not self.cfg.buy else +1   # selling pushes price down; buying up\n",
    "        # Permanent impact\n",
    "        dp_perm = sign * self.cfg.kappa * q_exec\n",
    "        self.perm_impact_cum += dp_perm\n",
    "        # Temporary impact (execution price vs current mid)\n",
    "        tmp_imp = self.cfg.eta * (q_exec / max(1.0, mkt_vol))  # scales with fraction of slice volume\n",
    "        micro_noise = np.random.normal(0, self.cfg.noise_sd)\n",
    "\n",
    "        # Midprice evolution (exo + permanent impact)\n",
    "        mid_no_impact = self.mid_exo[self.t]\n",
    "        self.p = max(0.01, mid_no_impact + self.perm_impact_cum + micro_noise)\n",
    "\n",
    "        # Realized execution price (we cross the spread + temp impact)\n",
    "        if not self.cfg.buy:\n",
    "            # SELL: execute near bid -> mid - half_spread - tmp_imp\n",
    "            px_exec = self.p - self.half_spread - tmp_imp * self.p\n",
    "        else:\n",
    "            # BUY: execute near ask -> mid + half_spread + tmp_imp\n",
    "            px_exec = self.p + self.half_spread + tmp_imp * self.p\n",
    "\n",
    "        # Cashflow & inventory update\n",
    "        cashflow = px_exec * q_exec * (+1 if not self.cfg.buy else -1)  # selling adds cash, buying uses cash\n",
    "        self.q_rem -= q_exec\n",
    "        self.t += 1\n",
    "        self.last_participation = part\n",
    "\n",
    "        # Reward shaping: negative implementation shortfall increment\n",
    "        # IS = (arrival_mid - realized_px) * q for SELL (opposite for BUY)\n",
    "        arrival = self.cfg.p0\n",
    "        if not self.cfg.buy:\n",
    "            inc_cost = (arrival - px_exec) * q_exec\n",
    "        else:\n",
    "            inc_cost = (px_exec - arrival) * q_exec\n",
    "\n",
    "        reward = -inc_cost / max(1.0, self.cfg.Q0 * self.cfg.p0)  # scale by notional\n",
    "        done = (self.q_rem <= 1e-8) or (self.t >= self.cfg.T)\n",
    "\n",
    "        # Apply terminal penalty if time ends with unexecuted inventory\n",
    "        if done and self.q_rem > 0:\n",
    "            # Force market order for remaining at worse price (spread + big temp impact)\n",
    "            pen_tmp = self.cfg.eta * (self.q_rem / (self.market_vol[self.t-1] if self.t>0 else self.cfg.vol_mu)) * 5.0\n",
    "            worst_px = (self.p - self.half_spread - pen_tmp * self.p) if not self.cfg.buy else (self.p + self.half_spread + pen_tmp * self.p)\n",
    "            rem_cost = ((arrival - worst_px) if not self.cfg.buy else (worst_px - arrival)) * self.q_rem\n",
    "            reward += - rem_cost / max(1.0, self.cfg.Q0 * self.cfg.p0)\n",
    "            self.q_rem = 0.0\n",
    "\n",
    "        return self._state(), float(reward), done, {\n",
    "            \"q_exec\": q_exec, \"px_exec\": float(px_exec), \"mid\": float(self.p), \"part\": float(part)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e816e5fb",
   "metadata": {},
   "source": [
    "## 3) Baselines: TWAP & POV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c5414a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_policy(env: ExecEnv, policy_fn, episodes=50):\n",
    "    rewards, costs = [], []\n",
    "    for _ in range(episodes):\n",
    "        s = env.reset()\n",
    "        total_r, total_cost = 0.0, 0.0\n",
    "        while True:\n",
    "            a = policy_fn(env, s)\n",
    "            s, r, done, info = env.step(a)\n",
    "            total_r += r\n",
    "            # translate reward back to cost\n",
    "            total_cost += -r * (env.cfg.Q0 * env.cfg.p0)\n",
    "            if done:\n",
    "                break\n",
    "        rewards.append(total_r)\n",
    "        costs.append(total_cost)\n",
    "    return np.array(rewards), np.array(costs)\n",
    "\n",
    "def twap_policy(env: ExecEnv, s):\n",
    "    # target equal shares each slice -> choose participation to hit target\n",
    "    slices_left = max(1, env.cfg.T - env.t)\n",
    "    target_q = env.q_rem / slices_left\n",
    "    desired_part = min(env.cfg.max_participation, target_q / env.market_vol[env.t])\n",
    "    # map to discrete action\n",
    "    a = int(round(desired_part / env.cfg.max_participation * (env.action_space-1)))\n",
    "    return np.clip(a, 0, env.action_space-1)\n",
    "\n",
    "def pov_policy(env: ExecEnv, s, pov=0.1):\n",
    "    desired_part = min(env.cfg.max_participation, pov)\n",
    "    a = int(round(desired_part / env.cfg.max_participation * (env.action_space-1)))\n",
    "    return np.clip(a, 0, env.action_space-1)\n",
    "\n",
    "cfg = ExecConfig()\n",
    "env = ExecEnv(cfg)\n",
    "\n",
    "r_twap, c_twap = run_policy(env, lambda e,s: twap_policy(e,s), episodes=30)\n",
    "r_pov,  c_pov  = run_policy(env, lambda e,s: pov_policy(e,s, pov=0.1), episodes=30)\n",
    "\n",
    "print(\"TWAP  : mean cost $\", np.mean(c_twap))\n",
    "print(\"POV10%: mean cost $\", np.mean(c_pov))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39caf94",
   "metadata": {},
   "source": [
    "## 4) DQN Agent (PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c373e122",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class QNet(nn.Module):\n",
    "    def __init__(self, state_dim, n_actions):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64), nn.ReLU(),\n",
    "            nn.Linear(64, 64), nn.ReLU(),\n",
    "            nn.Linear(64, n_actions)\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, cap=50_000):\n",
    "        self.s, self.a, self.r, self.ns, self.d = [], [], [], [], []\n",
    "        self.cap = cap\n",
    "        self.ptr = 0\n",
    "    def push(self, s,a,r,ns,d):\n",
    "        if len(self.s) < self.cap:\n",
    "            self.s.append(s); self.a.append(a); self.r.append(r); self.ns.append(ns); self.d.append(d)\n",
    "        else:\n",
    "            i = self.ptr % self.cap\n",
    "            self.s[i]=s; self.a[i]=a; self.r[i]=r; self.ns[i]=ns; self.d[i]=d\n",
    "            self.ptr += 1\n",
    "    def sample(self, bs=64):\n",
    "        idx = np.random.randint(0, len(self.s), size=bs)\n",
    "        return (\n",
    "            torch.tensor(np.array([self.s[i] for i in idx]), dtype=torch.float32, device=device),\n",
    "            torch.tensor(np.array([self.a[i] for i in idx]), dtype=torch.int64, device=device),\n",
    "            torch.tensor(np.array([self.r[i] for i in idx]), dtype=torch.float32, device=device),\n",
    "            torch.tensor(np.array([self.ns[i] for i in idx]), dtype=torch.float32, device=device),\n",
    "            torch.tensor(np.array([self.d[i] for i in idx]), dtype=torch.float32, device=device),\n",
    "        )\n",
    "    def __len__(self): return len(self.s)\n",
    "\n",
    "def train_dqn(env: ExecEnv, episodes=300, gamma=0.99, lr=1e-3, eps_start=1.0, eps_end=0.05, eps_decay=0.995, target_sync=200):\n",
    "    state_dim = len(env._state())\n",
    "    n_actions = env.action_space\n",
    "    q = QNet(state_dim, n_actions).to(device)\n",
    "    qt = QNet(state_dim, n_actions).to(device)\n",
    "    qt.load_state_dict(q.state_dict())\n",
    "    opt = optim.Adam(q.parameters(), lr=lr)\n",
    "    buf = ReplayBuffer(100_000)\n",
    "\n",
    "    eps = eps_start\n",
    "    rewards = []\n",
    "\n",
    "    step = 0\n",
    "    for ep in range(episodes):\n",
    "        s = env.reset()\n",
    "        ep_reward = 0.0\n",
    "        while True:\n",
    "            step += 1\n",
    "            if np.random.rand() < eps:\n",
    "                a = np.random.randint(0, n_actions)\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    a = q(torch.tensor(s, device=device)).argmax().item()\n",
    "            ns, r, done, info = env.step(a)\n",
    "            buf.push(s,a,r,ns,done)\n",
    "            s = ns\n",
    "            ep_reward += r\n",
    "\n",
    "            if len(buf) >= 512:\n",
    "                S,A,R,NS,D = buf.sample(128)\n",
    "                with torch.no_grad():\n",
    "                    max_next = qt(NS).max(dim=1)[0]\n",
    "                    y = R + gamma * (1 - D) * max_next\n",
    "                q_sa = q(S).gather(1, A.view(-1,1)).squeeze(1)\n",
    "                loss = F.mse_loss(q_sa, y)\n",
    "                opt.zero_grad(); loss.backward(); opt.step()\n",
    "\n",
    "                if step % target_sync == 0:\n",
    "                    qt.load_state_dict(q.state_dict())\n",
    "\n",
    "            if done: break\n",
    "\n",
    "        rewards.append(ep_reward)\n",
    "        eps = max(eps_end, eps * eps_decay)\n",
    "        if (ep+1) % 20 == 0:\n",
    "            print(f\"Episode {ep+1}/{episodes}  avgR(last20)={np.mean(rewards[-20:]):.6f}  eps={eps:.3f}\")\n",
    "    return q, rewards\n",
    "\n",
    "cfg = ExecConfig()\n",
    "env = ExecEnv(cfg)\n",
    "qnet, rew_hist = train_dqn(env, episodes=200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd223114",
   "metadata": {},
   "source": [
    "## 5) Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe24510",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(10,3.5))\n",
    "plt.plot(pd.Series(rew_hist).rolling(10).mean())\n",
    "plt.title(\"DQN â€” Rolling Mean Reward (10 ep)\")\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f75e5a",
   "metadata": {},
   "source": [
    "## 6) Evaluation vs Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63e1ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def eval_policy(env: ExecEnv, policy, episodes=100):\n",
    "    costs = []\n",
    "    for _ in range(episodes):\n",
    "        s = env.reset()\n",
    "        total_cost = 0.0\n",
    "        while True:\n",
    "            a = policy(env, s)\n",
    "            s, r, done, info = env.step(a)\n",
    "            total_cost += -r * (env.cfg.Q0 * env.cfg.p0)\n",
    "            if done: break\n",
    "        costs.append(total_cost)\n",
    "    return np.array(costs)\n",
    "\n",
    "def policy_from_q(qnet: QNet):\n",
    "    def fn(env, s):\n",
    "        with torch.no_grad():\n",
    "            t = torch.tensor(s, dtype=torch.float32, device=device)\n",
    "            return int(qnet(t).argmax().item())\n",
    "    return fn\n",
    "\n",
    "env = ExecEnv(cfg)\n",
    "cost_rl  = eval_policy(env, policy_from_q(qnet), episodes=100)\n",
    "cost_twap = eval_policy(env, lambda e,s: twap_policy(e,s), episodes=100)\n",
    "cost_pov  = eval_policy(env, lambda e,s: pov_policy(e,s, pov=0.1), episodes=100)\n",
    "\n",
    "print(\"Mean costs ($): RL={:.2f}  TWAP={:.2f}  POV10={:.2f}\".format(cost_rl.mean(), cost_twap.mean(), cost_pov.mean()))\n",
    "\n",
    "plt.figure(figsize=(10,3.5))\n",
    "plt.hist(cost_rl, bins=30, alpha=0.6, label=\"RL\")\n",
    "plt.hist(cost_twap, bins=30, alpha=0.6, label=\"TWAP\")\n",
    "plt.hist(cost_pov, bins=30, alpha=0.6, label=\"POV10%\")\n",
    "plt.title(\"Distribution of Implementation Shortfall Costs\")\n",
    "plt.legend(); plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795e5313",
   "metadata": {},
   "source": [
    "## 7) Example Episode â€” Inventory & Participation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba74186c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rollout(env: ExecEnv, policy, record=True):\n",
    "    s = env.reset()\n",
    "    rec = {\"t\": [], \"q_rem\": [], \"part\": [], \"px\": [], \"exec_px\": [], \"mid\": []}\n",
    "    total_cost = 0.0\n",
    "    while True:\n",
    "        a = policy(env, s)\n",
    "        ns, r, done, info = env.step(a)\n",
    "        if record:\n",
    "            rec[\"t\"].append(env.t)\n",
    "            rec[\"q_rem\"].append(env.q_rem)\n",
    "            rec[\"part\"].append(info[\"part\"])\n",
    "            rec[\"mid\"].append(info[\"mid\"])\n",
    "            rec[\"exec_px\"].append(info[\"px_exec\"])\n",
    "        s = ns\n",
    "        total_cost += -r * (env.cfg.Q0 * env.cfg.p0)\n",
    "        if done: break\n",
    "    return total_cost, pd.DataFrame(rec)\n",
    "\n",
    "env = ExecEnv(cfg)\n",
    "cost, df = rollout(env, policy_from_q(qnet))\n",
    "\n",
    "fig, ax = plt.subplots(2,1, figsize=(10,6))\n",
    "ax[0].plot(df[\"t\"], df[\"q_rem\"]); ax[0].set_title(\"Remaining Inventory\")\n",
    "ax[1].plot(df[\"t\"], df[\"part\"]); ax[1].set_title(\"Participation per Slice\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.plot(df[\"t\"], df[\"mid\"], label=\"Mid\")\n",
    "plt.plot(df[\"t\"], df[\"exec_px\"], label=\"Exec Px\")\n",
    "plt.title(\"Prices During Episode\")\n",
    "plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "print(\"Example episode cost ($):\", cost)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c4b1b4",
   "metadata": {},
   "source": [
    "\n",
    "## 8) Extensions (easy to add)\n",
    "- Add **stochastic spreads** and **venue selection** (extend action space).\n",
    "- Penalty on **aggressiveness** or **variance of participation**.\n",
    "- Replace DQN with **PPO** or **DDQN**.\n",
    "- Train on **buy** as well as **sell** programs (flip `buy=True`).\n",
    "- Plug real **tape/LOB features** (depth imbalance, recent volume, toxicity).\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
