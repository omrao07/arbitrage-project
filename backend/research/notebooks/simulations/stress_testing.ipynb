{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51ca5641",
   "metadata": {},
   "source": [
    "\n",
    "# ðŸ§¨ Stress Testing Workbench\n",
    "\n",
    "_Date generated: 2025-09-03_\n",
    "\n",
    "A selfâ€‘contained notebook to run **portfolio stress tests** with factor and idiosyncratic shocks, \n",
    "**correlation spikes**, and **liquidity/impact** overlays. Uses your CSVs if present, else synthetic data.\n",
    "\n",
    "**What you get**\n",
    "- Load portfolio, prices/returns, and optional factor exposures.\n",
    "- Scenario library: historical (1987, GFC, COVID-19), macro shocks, bespoke factor & vol shocks.\n",
    "- Liquidity layer: spread widening, volume drought â†’ slippage & capacity hit.\n",
    "- Risk: instantaneous **PnL**, **VaR/ES**, heatmaps, tornado charts, waterfall of worst scenarios.\n",
    "- Batch mode: run suite & export `reports/stress_summary.csv`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e99ad2b",
   "metadata": {},
   "source": [
    "## 0) Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf76815",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Optional inputs (CSV). If missing, synthetic will be created.\n",
    "PATH_RETURNS = \"data/returns.csv\"   # wide: date, T1, T2, ...\n",
    "PATH_WEIGHTS = \"data/weights.csv\"   # cols: ticker, weight\n",
    "PATH_FACTORS = \"data/factors.csv\"   # wide: date, MKT, RATE, CRUDE, USD, ... (optional)\n",
    "PATH_LOADINGS = \"data/loadings.csv\" # asset factor loadings: ticker, factor, beta (optional)\n",
    "\n",
    "# Defaults\n",
    "N_ASSETS = 12\n",
    "N_DAYS = 800\n",
    "RISK_FREE = 0.01\n",
    "\n",
    "# Liquidity / cost knobs (bps)\n",
    "BASE_SPREAD_BPS = 5.0\n",
    "CRISIS_SPREAD_MULT = 3.0\n",
    "TURNOVER_PER_SCENARIO = 0.5     # fraction of book turned in stress (for slippage proxy)\n",
    "IMPACT_BPS_PER_10VOL = 4.0      # extra bps per +10% vol spike\n",
    "\n",
    "SEED = 7\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15be1691",
   "metadata": {},
   "source": [
    "## 1) Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523e1cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, math, warnings\n",
    "from typing import Dict, List, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.options.display.float_format = \"{:,.6f}\".format\n",
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "def drawdown_curve(nav: pd.Series) -> pd.Series:\n",
    "    return nav / nav.cummax() - 1.0\n",
    "\n",
    "def pct(x): return 100.0 * x\n",
    "\n",
    "def ensure_dir(p): \n",
    "    os.makedirs(os.path.dirname(p), exist_ok=True)\n",
    "    return p\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb07e2c6",
   "metadata": {},
   "source": [
    "## 2) Load Data (CSV or Synthetic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831f1b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_returns(path=PATH_RETURNS, n_assets=N_ASSETS, n_days=N_DAYS):\n",
    "    if os.path.exists(path):\n",
    "        return pd.read_csv(path, parse_dates=[\"date\"]).set_index(\"date\").sort_index()\n",
    "    dates = pd.bdate_range(\"2022-01-01\", periods=n_days)\n",
    "    mu = rng.normal(0.0004, 0.0002, n_assets)\n",
    "    sd = rng.uniform(0.01, 0.025, n_assets)\n",
    "    data = [rng.normal(mu[i], sd[i], len(dates)) for i in range(n_assets)]\n",
    "    cols = [f\"A{i:02d}\" for i in range(n_assets)]\n",
    "    return pd.DataFrame(np.array(data).T, index=dates, columns=cols)\n",
    "\n",
    "def load_weights(path=PATH_WEIGHTS, tickers=None):\n",
    "    if os.path.exists(path):\n",
    "        df = pd.read_csv(path)\n",
    "        w = df.set_index(\"ticker\")[\"weight\"]\n",
    "        return w / w.abs().sum()\n",
    "    # Equal-weight default long-only\n",
    "    w = pd.Series(1.0, index=tickers) / len(tickers) # type: ignore\n",
    "    return w\n",
    "\n",
    "def load_factors(path=PATH_FACTORS, start=None, end=None):\n",
    "    if os.path.exists(path):\n",
    "        f = pd.read_csv(path, parse_dates=[\"date\"]).set_index(\"date\").sort_index()\n",
    "        return f.loc[start:end]\n",
    "    # Synthetic factors\n",
    "    idx = pd.bdate_range(start, end)\n",
    "    factors = pd.DataFrame({\n",
    "        \"MKT\": rng.normal(0.0003, 0.01, len(idx)),\n",
    "        \"RATE\": rng.normal(0.0, 0.005, len(idx)),\n",
    "        \"CRUDE\": rng.normal(0.0, 0.012, len(idx)),\n",
    "        \"USD\": rng.normal(0.0, 0.006, len(idx)),\n",
    "        \"VOL\": np.abs(rng.normal(0.0, 0.01, len(idx)))\n",
    "    }, index=idx)\n",
    "    return factors\n",
    "\n",
    "def load_loadings(path=PATH_LOADINGS, tickers=None, factors=None):\n",
    "    if os.path.exists(path):\n",
    "        df = pd.read_csv(path)\n",
    "        piv = df.pivot(index=\"ticker\", columns=\"factor\", values=\"beta\").reindex(tickers).fillna(0.0)\n",
    "        return piv\n",
    "    # Random loadings for synthetic\n",
    "    betas = pd.DataFrame(rng.normal(0, 0.7, size=(len(tickers), len(factors.columns))),# type: ignore\n",
    "                         index=tickers, columns=factors.columns)# type: ignore\n",
    "    # VOL factor loading is non-negative proxy\n",
    "    betas[\"VOL\"] = np.abs(betas[\"VOL\"])\n",
    "    return betas\n",
    "\n",
    "rets = load_returns()\n",
    "weights = load_weights(tickers=rets.columns)\n",
    "factors = load_factors(start=rets.index.min(), end=rets.index.max())\n",
    "betas = load_loadings(tickers=rets.columns, factors=factors)\n",
    "\n",
    "rets.head(), weights.head(), factors.head(), betas.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95130767",
   "metadata": {},
   "source": [
    "## 3) Baseline Portfolio Risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3e8447",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "port_ret = (rets * weights.reindex(rets.columns)).sum(axis=1)\n",
    "nav = (1 + port_ret).cumprod()\n",
    "baseline = {\n",
    "    \"CAGR\": nav.iloc[-1]**(252/len(nav)) - 1,\n",
    "    \"Vol\": port_ret.std()*np.sqrt(252),\n",
    "    \"Sharpe\": (port_ret.mean()/port_ret.std())*np.sqrt(252) if port_ret.std()>0 else np.nan,\n",
    "    \"MaxDD\": drawdown_curve(nav).min()\n",
    "}\n",
    "pd.Series(baseline)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35e6aff",
   "metadata": {},
   "source": [
    "## 4) Scenario Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dad65ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def shock_vector(**kwargs):\n",
    "    # keys over factor names; values in return units for a 1-day instantaneous shock\n",
    "    return pd.Series(kwargs).reindex(betas.columns).fillna(0.0)\n",
    "\n",
    "SCENARIOS = {\n",
    "    \"1987_Crash\": shock_vector(MKT=-0.20, VOL=+0.20, USD=+0.02),\n",
    "    \"GFC_Liquidity\": shock_vector(MKT=-0.08, RATE=-0.02, CRUDE=-0.06, VOL=+0.10),\n",
    "    \"COVID_Selloff\": shock_vector(MKT=-0.12, RATE=-0.01, CRUDE=-0.10, USD=+0.03, VOL=+0.15),\n",
    "    \"Inflation_Spike\": shock_vector(MKT=-0.04, RATE=+0.03, CRUDE=+0.05, USD=+0.01, VOL=+0.05),\n",
    "    \"Energy_OilShock\": shock_vector(CRUDE=+0.12, MKT=-0.03, RATE=+0.005, VOL=+0.03),\n",
    "    \"USD_Crush\": shock_vector(USD=+0.05, MKT=-0.02),\n",
    "    \"Rates_Tantrum\": shock_vector(RATE=+0.04, MKT=-0.05, VOL=+0.06),\n",
    "}\n",
    "\n",
    "# Bespoke grid generator (e.g., +/- X% on each factor)\n",
    "def grid_scenarios(level=0.05):\n",
    "    out = {}\n",
    "    for f in betas.columns:\n",
    "        out[f\"+{f}_{level:.1%}\"] = shock_vector(**{f: +level})\n",
    "        out[f\"-{f}_{level:.1%}\"] = shock_vector(**{f: -level})\n",
    "    return out\n",
    "\n",
    "SCENARIOS.update(grid_scenarios(0.03))\n",
    "len(SCENARIOS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd89a6e",
   "metadata": {},
   "source": [
    "## 5) Liquidity & Cost Overlay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc993077",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def liquidity_cost(spread_bps=BASE_SPREAD_BPS, spread_mult=1.0, turnover=TURNOVER_PER_SCENARIO, vol_spike=0.0):\n",
    "    spread = spread_bps * spread_mult\n",
    "    slip_bps = spread * turnover\n",
    "    impact_bps = IMPACT_BPS_PER_10VOL * (vol_spike/0.10)\n",
    "    return (slip_bps + impact_bps) / 1e4  # to return units\n",
    "\n",
    "def apply_scenario(shock_name, svec: pd.Series, weights: pd.Series, betas: pd.DataFrame):\n",
    "    # Factor-driven instantaneous PnL: asset_ret = betas @ shock + idio (ignore idio here)\n",
    "    asset_shock = betas.values @ svec.values\n",
    "    asset_shock = pd.Series(asset_shock, index=betas.index)\n",
    "    pnl_no_cost = float((asset_shock * weights).sum())\n",
    "    # liquidity penalty: spread widening & vol spike assumption from 'VOL' factor\n",
    "    vol_spike = max(0.0, float(svec.get(\"VOL\", 0.0)))\n",
    "    spread_mult = CRISIS_SPREAD_MULT if vol_spike>0 else 1.0\n",
    "    cost = liquidity_cost(spread_mult=spread_mult, vol_spike=vol_spike)\n",
    "    pnl_after_cost = pnl_no_cost - cost\n",
    "    out = {\n",
    "        \"scenario\": shock_name,\n",
    "        \"gross_pnl\": pnl_no_cost,\n",
    "        \"liq_cost\": -cost,\n",
    "        \"net_pnl\": pnl_after_cost,\n",
    "        \"vol_spike\": vol_spike,\n",
    "        \"spread_mult\": spread_mult\n",
    "    }\n",
    "    return out, asset_shock\n",
    "\n",
    "# Run all scenarios\n",
    "rows = []\n",
    "asset_impacts = {}\n",
    "for name, svec in SCENARIOS.items():\n",
    "    out, ashock = apply_scenario(name, svec, weights, betas)\n",
    "    rows.append(out); asset_impacts[name] = ashock\n",
    "\n",
    "stress_df = pd.DataFrame(rows).set_index(\"scenario\").sort_values(\"net_pnl\")\n",
    "stress_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d37e3d",
   "metadata": {},
   "source": [
    "## 6) VaR / ES (Historical) & Scenario Overlay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa78c057",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hist = (rets * weights.reindex(rets.columns)).sum(axis=1).dropna()\n",
    "alpha = 0.99\n",
    "VaR = -np.percentile(hist, (1-alpha)*100)\n",
    "ES = -hist[hist <= -VaR].mean()\n",
    "\n",
    "risk_table = pd.DataFrame({\n",
    "    \"Hist_VaR_99\": [VaR],\n",
    "    \"Hist_ES_99\": [ES],\n",
    "    \"Worst_Stress_Net\": [stress_df[\"net_pnl\"].min()],\n",
    "    \"Median_Stress_Net\": [stress_df[\"net_pnl\"].median()]\n",
    "})\n",
    "risk_table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2256e9",
   "metadata": {},
   "source": [
    "## 7) Visuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c828ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tornado (sorted net pnl)\n",
    "sorted_net = stress_df[\"net_pnl\"].sort_values()\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.barh(sorted_net.index, sorted_net.values)\n",
    "plt.title(\"Scenario Net PnL (Tornado)\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# Heatmap of asset impacts for worst scenarios\n",
    "worst_names = stress_df.nsmallest(6, \"net_pnl\").index.tolist()\n",
    "heat = pd.DataFrame({k: asset_impacts[k] for k in worst_names})\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.imshow(heat.values.T, aspect='auto')\n",
    "plt.yticks(range(len(worst_names)), worst_names)\n",
    "plt.xticks(range(len(heat.index)), heat.index, rotation=90)\n",
    "plt.title(\"Asset Impact (Worst Scenarios)\")\n",
    "plt.colorbar(); plt.tight_layout(); plt.show()\n",
    "\n",
    "# Waterfall for worst scenario\n",
    "wname = worst_names[0]\n",
    "gross = stress_df.loc[wname, \"gross_pnl\"]\n",
    "cost = stress_df.loc[wname, \"liq_cost\"]\n",
    "net = stress_df.loc[wname, \"net_pnl\"]\n",
    "plt.figure(figsize=(8,3.2))\n",
    "vals = [0, gross, gross+cost, net]  # start at 0 -> gross -> +cost -> net\n",
    "labels = [\"Start\", \"Gross\", \"Cost\", \"Net\"]\n",
    "plt.plot([0,1,2,3], vals, marker=\"o\")\n",
    "for i,(x,y) in enumerate(zip(range(4), vals)):\n",
    "    plt.text(x, y, f\"{y:.2%}\", ha=\"center\", va=\"bottom\")\n",
    "plt.xticks(range(4), labels)\n",
    "plt.title(f\"Waterfall â€” {wname}\")\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a4dfe0",
   "metadata": {},
   "source": [
    "## 8) Export Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94a8c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "summary = stress_df.copy()\n",
    "summary[\"rank\"] = summary[\"net_pnl\"].rank(ascending=True, method=\"dense\")\n",
    "ensure_dir(\"reports/stress_summary.csv\")\n",
    "summary.to_csv(\"reports/stress_summary.csv\")\n",
    "\"reports/stress_summary.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023aa437",
   "metadata": {},
   "source": [
    "## 9) Sensitivity Sweeps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed58c1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sweep_factor(factor, lows=-0.10, highs=0.10, steps=21):\n",
    "    xs = np.linspace(lows, highs, steps)\n",
    "    results = []\n",
    "    for x in xs:\n",
    "        s = shock_vector(**{factor: x})\n",
    "        out, _ = apply_scenario(f\"{factor}@{x:.1%}\", s, weights, betas)\n",
    "        results.append((x, out[\"net_pnl\"]))\n",
    "    df = pd.DataFrame(results, columns=[\"shock\",\"net_pnl\"])\n",
    "    return df\n",
    "\n",
    "sv = sweep_factor(\"MKT\")\n",
    "plt.figure(figsize=(8,3))\n",
    "plt.plot(pct(sv[\"shock\"]), pct(sv[\"net_pnl\"]))\n",
    "plt.axhline(0, color=\"black\", lw=1)\n",
    "plt.xlabel(\"MKT shock (%)\"); plt.ylabel(\"Net PnL (%)\")\n",
    "plt.title(\"Sensitivity â€” Market Shock\")\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
