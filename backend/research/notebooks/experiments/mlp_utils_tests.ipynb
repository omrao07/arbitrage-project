{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7682caf0",
   "metadata": {},
   "source": [
    "\n",
    "# ✅ MLP Utils — Tests & Diagnostics\n",
    "\n",
    "This notebook runs a **battery of tests** for your `mlp_utils.py` helpers.  \n",
    "It will try to import `backend/ml/mlp_utils.py`. If it can't find it, it creates a **compatible fallback** so the tests still run.\n",
    "\n",
    "**What we test**\n",
    "- Import & environment sanity (CPU/GPU availability)\n",
    "- Model construction (shapes, parameter counts)\n",
    "- Forward/Backward passes (no NaNs, gradients finite)\n",
    "- Training on synthetic data (loss goes down, accuracy up)\n",
    "- Reproducibility (fixed seeds)\n",
    "- Save/Load round-trip equivalence\n",
    "- Simple learning rate & batch size sweeps (optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b7bdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, sys, math, json, copy, time, pathlib, tempfile\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# PyTorch\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "    from torch.utils.data import TensorDataset, DataLoader\n",
    "    TORCH_OK = True\n",
    "except Exception as e:\n",
    "    TORCH_OK = False\n",
    "    print(\"[warn] PyTorch not available:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67a748e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def set_seed(seed=42):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if 'torch' in sys.modules:\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(1337)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780e46f7",
   "metadata": {},
   "source": [
    "## 1) Import `mlp_utils` (or provide fallback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4327717c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MLP_UTILS = None\n",
    "ERR = None\n",
    "\n",
    "if TORCH_OK:\n",
    "    # Try common repo locations\n",
    "    candidate_paths = [\n",
    "        \".\", \"backend\", \"backend/ml\", \"ml\"\n",
    "    ]\n",
    "    for cp in candidate_paths:\n",
    "        p = pathlib.Path(cp)\n",
    "        if (p / \"mlp_utils.py\").exists():\n",
    "            sys.path.insert(0, str(p.resolve()))\n",
    "            try:\n",
    "                import mlp_utils as MLP_UTILS # type: ignore\n",
    "                break\n",
    "            except Exception as e:\n",
    "                ERR = e\n",
    "\n",
    "if MLP_UTILS is None and TORCH_OK:\n",
    "    # Fallback minimal implementation (API compatible with tests)\n",
    "    class MLP(nn.Module):\n",
    "        def __init__(self, in_dim, hidden, out_dim, dropout=0.0):\n",
    "            super().__init__()\n",
    "            layers = []\n",
    "            last = in_dim\n",
    "            for h in hidden:\n",
    "                layers += [nn.Linear(last, h), nn.ReLU(), nn.Dropout(dropout)]\n",
    "                last = h\n",
    "            layers.append(nn.Linear(last, out_dim))\n",
    "            self.net = nn.Sequential(*layers)\n",
    "        def forward(self, x): return self.net(x)\n",
    "    def make_mlp(in_dim, hidden, out_dim, dropout=0.0):\n",
    "        return MLP(in_dim, hidden, out_dim, dropout)\n",
    "    def count_params(model):\n",
    "        return sum(p.numel() for p in model.parameters())\n",
    "    def save_model(model, path):\n",
    "        torch.save(model.state_dict(), path)\n",
    "    def load_model(model, path, map_location=\"cpu\"):\n",
    "        sd = torch.load(path, map_location=map_location)\n",
    "        model.load_state_dict(sd)\n",
    "        return model\n",
    "    def train_one_epoch(model, loader, opt, device=\"cpu\", scheduler=None):\n",
    "        model.train()\n",
    "        total = 0.0\n",
    "        for xb, yb in loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            logits = model(xb)\n",
    "            loss = F.cross_entropy(logits, yb)\n",
    "            loss.backward()\n",
    "            # gradient sanity check (finite)\n",
    "            gnorm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "            opt.step()\n",
    "            if scheduler: scheduler.step()\n",
    "            total += float(loss.detach().cpu())\n",
    "        return total / max(1, len(loader))\n",
    "    def evaluate(model, loader, device=\"cpu\"):\n",
    "        model.eval()\n",
    "        n, correct, total_loss = 0, 0, 0.0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                logits = model(xb)\n",
    "                total_loss += float(F.cross_entropy(logits, yb).cpu())\n",
    "                pred = logits.argmax(dim=1)\n",
    "                correct += int((pred == yb).sum().cpu())\n",
    "                n += yb.numel()\n",
    "        return {\"loss\": total_loss / max(1, len(loader)), \"acc\": correct / max(1, n)}\n",
    "    # Bundle\n",
    "    class _NS: pass\n",
    "    MLP_UTILS = _NS()\n",
    "    MLP_UTILS.make_mlp = make_mlp # type: ignore\n",
    "    MLP_UTILS.count_params = count_params # type: ignore\n",
    "    MLP_UTILS.save_model = save_model # type: ignore\n",
    "    MLP_UTILS.load_model = load_model # type: ignore\n",
    "    MLP_UTILS.train_one_epoch = train_one_epoch # type: ignore\n",
    "    MLP_UTILS.evaluate = evaluate # type: ignore\n",
    "\n",
    "print(\"mlp_utils source:\", \"fallback\" if ERR or (MLP_UTILS and not hasattr(MLP_UTILS, \"__file__\")) else getattr(MLP_UTILS, \"__file__\", \"unknown\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06c5b17",
   "metadata": {},
   "source": [
    "## 2) Environment & Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e9a25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = \"cuda\" if TORCH_OK and torch.cuda.is_available() else \"cpu\"\n",
    "print(\"PyTorch:\", TORCH_OK, \"| Device:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a305342b",
   "metadata": {},
   "source": [
    "## 3) Synthetic Classification Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0a05a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not TORCH_OK:\n",
    "    raise RuntimeError(\"PyTorch is required for these tests.\")\n",
    "\n",
    "set_seed(123)\n",
    "n_classes = 3\n",
    "n_features = 16\n",
    "n_train, n_val = 4000, 1000\n",
    "\n",
    "W_true = torch.randn(n_features, n_classes)\n",
    "b_true = torch.randn(n_classes)\n",
    "\n",
    "X_tr = torch.randn(n_train, n_features)\n",
    "logits_tr = X_tr @ W_true + b_true\n",
    "y_tr = logits_tr.argmax(dim=1)\n",
    "\n",
    "X_va = torch.randn(n_val, n_features)\n",
    "logits_va = X_va @ W_true + b_true\n",
    "y_va = logits_va.argmax(dim=1)\n",
    "\n",
    "train_ds = TensorDataset(X_tr, y_tr)\n",
    "val_ds   = TensorDataset(X_va, y_va)\n",
    "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, drop_last=False)\n",
    "val_loader   = DataLoader(val_ds, batch_size=256, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a01730",
   "metadata": {},
   "source": [
    "## 4) Model Build — Shapes & Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbd2951",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = MLP_UTILS.make_mlp(in_dim=n_features, hidden=[64, 32], out_dim=n_classes, dropout=0.1).to(device) # type: ignore\n",
    "x = next(iter(train_loader))[0].to(device)\n",
    "out = model(x)\n",
    "assert out.shape == (x.shape[0], n_classes), f\"Unexpected output shape: {out.shape}\"\n",
    "params = MLP_UTILS.count_params(model) # type: ignore\n",
    "print(\"Param count:\", params)\n",
    "assert params > 0, \"Parameter count should be > 0\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b1e9fd",
   "metadata": {},
   "source": [
    "## 5) Training Loop — Loss Should Decrease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e765322a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=3e-3, weight_decay=1e-2)\n",
    "history = {\"train_loss\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "\n",
    "epochs = 12\n",
    "best = 1e9\n",
    "for ep in range(1, epochs+1):\n",
    "    tl = MLP_UTILS.train_one_epoch(model, train_loader, opt, device=device) # type: ignore\n",
    "    ev = MLP_UTILS.evaluate(model, val_loader, device=device) # type: ignore\n",
    "    history[\"train_loss\"].append(tl)\n",
    "    history[\"val_loss\"].append(ev[\"loss\"])\n",
    "    history[\"val_acc\"].append(ev[\"acc\"])\n",
    "    best = min(best, ev[\"loss\"])\n",
    "    print(f\"ep {ep:02d} | train {tl:.4f} | val {ev['loss']:.4f} | acc {ev['acc']:.3f}\")\n",
    "    \n",
    "assert history[\"val_loss\"][0] > history[\"val_loss\"][-1] - 1e-6, \"Validation loss did not decrease\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b813e044",
   "metadata": {},
   "source": [
    "### Plot Loss Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7454f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(history[\"train_loss\"], label=\"train\")\n",
    "plt.plot(history[\"val_loss\"], label=\"val\")\n",
    "plt.title(\"Loss Curves\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ec8d30",
   "metadata": {},
   "source": [
    "## 6) Reproducibility — Fixed Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8870001d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_quick(seed=777):\n",
    "    set_seed(seed)\n",
    "    m = MLP_UTILS.make_mlp(n_features, [32], n_classes).to(device) # type: ignore\n",
    "    opt = torch.optim.SGD(m.parameters(), lr=1e-2)\n",
    "    for _ in range(3):\n",
    "        _ = MLP_UTILS.train_one_epoch(m, train_loader, opt, device=device) # type: ignore\n",
    "    ev = MLP_UTILS.evaluate(m, val_loader, device=device) # type: ignore\n",
    "    # Return a small summary vector\n",
    "    vec = torch.cat([p.detach().flatten()[:20].cpu() for p in m.parameters()])[:50].numpy()\n",
    "    return ev[\"loss\"], ev[\"acc\"], vec\n",
    "\n",
    "l1, a1, v1 = train_quick(seed=2024)\n",
    "l2, a2, v2 = train_quick(seed=2024)\n",
    "\n",
    "print(\"Repeat loss diff:\", abs(l1 - l2))\n",
    "print(\"Repeat acc  diff:\", abs(a1 - a2))\n",
    "print(\"Vec L2 diff    :\", float(np.linalg.norm(v1 - v2)))\n",
    "assert abs(l1 - l2) < 1e-4 and abs(a1 - a2) < 1e-4 and np.linalg.norm(v1 - v2) < 1e-6, \"Non-deterministic outcome with fixed seed\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2570fb09",
   "metadata": {},
   "source": [
    "## 7) Save / Load Round-Trip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe3373d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with tempfile.TemporaryDirectory() as td:\n",
    "    p = os.path.join(td, \"mlp.pt\")\n",
    "    MLP_UTILS.save_model(model, p) # type: ignore\n",
    "    m2 = MLP_UTILS.make_mlp(n_features, [64,32], n_classes).to(device) # type: ignore\n",
    "    MLP_UTILS.load_model(m2, p, map_location=device) # type: ignore\n",
    "    # Compare a forward sample\n",
    "    x = next(iter(val_loader))[0].to(device)[:8]\n",
    "    y1 = model(x).detach().cpu().numpy()\n",
    "    y2 = m2(x).detach().cpu().numpy()\n",
    "    diff = np.abs(y1 - y2).max()\n",
    "    print(\"Max forward diff after load:\", diff)\n",
    "    assert diff < 1e-6, \"Model weights did not round-trip exactly\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef40472",
   "metadata": {},
   "source": [
    "## 8) Gradient Finite & Norm Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0958e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for p in model.parameters():\n",
    "    assert torch.isfinite(p).all(), \"Parameter contains NaN/Inf\"\n",
    "    \n",
    "# Backprop one step\n",
    "xb, yb = next(iter(train_loader))\n",
    "xb, yb = xb.to(device), yb.to(device)\n",
    "opt.zero_grad(set_to_none=True)\n",
    "loss = F.cross_entropy(model(xb), yb)\n",
    "loss.backward()\n",
    "gmax = 0.0\n",
    "for p in model.parameters():\n",
    "    if p.grad is not None:\n",
    "        gmax = max(gmax, float(p.grad.detach().abs().max().cpu()))\n",
    "        assert torch.isfinite(p.grad).all(), \"Gradient contains NaN/Inf\"\n",
    "print(\"Max grad abs:\", gmax)\n",
    "assert gmax > 0.0, \"Zero gradients observed\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d103106",
   "metadata": {},
   "source": [
    "## 9) (Optional) Mini LR Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f667bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lrs = [1e-3, 3e-3, 1e-2]\n",
    "results = []\n",
    "for lr in lrs:\n",
    "    set_seed(99)\n",
    "    m = MLP_UTILS.make_mlp(n_features, [32], n_classes).to(device) # type: ignore\n",
    "    opt = torch.optim.Adam(m.parameters(), lr=lr)\n",
    "    tl = MLP_UTILS.train_one_epoch(m, train_loader, opt, device=device) # type: ignore\n",
    "    ev = MLP_UTILS.evaluate(m, val_loader, device=device) # type: ignore\n",
    "    results.append((lr, ev[\"loss\"], ev[\"acc\"]))\n",
    "\n",
    "for lr, vl, va in results:\n",
    "    print(f\"lr={lr:.0e}: val_loss={vl:.4f} acc={va:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
