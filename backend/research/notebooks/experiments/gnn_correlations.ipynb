{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e64c4dd",
   "metadata": {},
   "source": [
    "\n",
    "# ðŸ§  GNN-Style Correlation Analysis\n",
    "This notebook builds a **correlation graph** from asset returns and applies a lightweight, framework-free, **GNN-style message passing** to learn smoothed node embeddings and cluster structures.\n",
    "\n",
    "**What you'll do:**\n",
    "1. Load returns (from CSV if available, else synthesize).\n",
    "2. Compute a correlation matrix + build a weighted graph.\n",
    "3. Extract communities (sector-like clusters) & the MST backbone.\n",
    "4. Run a simple 2-layer message-passing network (no special libs).\n",
    "5. Visualize embeddings & cluster structure.\n",
    "\n",
    "> Drop a CSV at `data/returns.csv` with columns: `date, TICKER1, TICKER2, ...` (wide format). The code will detect and use it automatically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be970893",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['axes.grid'] = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a85b6d4",
   "metadata": {},
   "source": [
    "## 1) Load Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef7b270",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_returns(path: str = 'data/returns.csv', n_assets: int = 40, n_days: int = 400, seed: int = 42) -> pd.DataFrame:\n",
    "    \"\"\"Load daily returns from CSV if present, else synthesize a market + sector model.\"\"\"\n",
    "    if os.path.exists(path):\n",
    "        df = pd.read_csv(path, parse_dates=['date'])\n",
    "        df = df.set_index('date').sort_index()\n",
    "        # Infer returns if prices provided\n",
    "        if (df > 5).all().all():  # hacky: assume raw prices if big\n",
    "            df = df.pct_change().dropna()\n",
    "        return df\n",
    "    rng = np.random.default_rng(seed)\n",
    "    # Synthetic market + sectors\n",
    "    n_sectors = 5\n",
    "    sector_members = rng.integers(0, n_sectors, size=n_assets)\n",
    "    market = rng.normal(0, 0.008, size=(n_days, 1))\n",
    "    sector_factors = rng.normal(0, 0.01, size=(n_days, n_sectors))\n",
    "    idio = rng.normal(0, 0.012, size=(n_days, n_assets))\n",
    "    data = np.zeros((n_days, n_assets))\n",
    "    for j in range(n_assets):\n",
    "        s = sector_members[j]\n",
    "        data[:, j] = 0.4*market[:,0] + 0.4*sector_factors[:, s] + 0.2*idio[:, j]\n",
    "    dates = pd.date_range('2022-01-01', periods=n_days, freq='B')\n",
    "    cols = [f\"A{j:02d}\" for j in range(n_assets)]\n",
    "    return pd.DataFrame(data, index=dates, columns=cols)\n",
    "\n",
    "rets = load_returns()\n",
    "rets.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c27099",
   "metadata": {},
   "source": [
    "## 2) Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352959b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "corr = rets.corr()\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.imshow(corr, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.colorbar(label='Correlation')\n",
    "plt.title('Asset Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe21b0f1",
   "metadata": {},
   "source": [
    "## 3) Build Correlation Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f431f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_graph_from_corr(corr: pd.DataFrame, threshold: float = 0.3) -> nx.Graph:\n",
    "    \"\"\"Undirected graph where edges connect pairs with |corr| >= threshold. Weight = corr.\"\"\"\n",
    "    G = nx.Graph()\n",
    "    tickers = corr.columns.tolist()\n",
    "    G.add_nodes_from(tickers)\n",
    "    for i, a in enumerate(tickers):\n",
    "        for j in range(i+1, len(tickers)):\n",
    "            b = tickers[j]\n",
    "            w = float(corr.iloc[i, j]) # type: ignore\n",
    "            if abs(w) >= threshold:\n",
    "                G.add_edge(a, b, weight=w)\n",
    "    return G\n",
    "\n",
    "G = build_graph_from_corr(corr, threshold=0.35)\n",
    "print(nx.info(G))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d29a0b7",
   "metadata": {},
   "source": [
    "## 4) Community Detection (Greedy Modularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785e2bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "communities = list(nx.algorithms.community.greedy_modularity_communities(G, weight='weight')) # type: ignore\n",
    "comm_map = {}\n",
    "for idx, com in enumerate(communities):\n",
    "    for n in com:\n",
    "        comm_map[n] = idx\n",
    "\n",
    "print(f\"Found {len(communities)} communities.\")\n",
    "for i, cset in enumerate(communities[:5]):\n",
    "    print(f\"Community {i}: {len(cset)} nodes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fc66a7",
   "metadata": {},
   "source": [
    "## 5) Minimum Spanning Tree (MST) Backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b9c95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Transform correlation to distance: d = sqrt(2*(1 - corr))\n",
    "W = corr.copy()\n",
    "np.fill_diagonal(W.values, 1.0)\n",
    "D = np.sqrt(2.0*(1.0 - W.clip(-0.999, 0.999)))\n",
    "# Build complete graph with distances\n",
    "G_all = nx.Graph()\n",
    "for i, a in enumerate(corr.columns):\n",
    "    for j in range(i+1, len(corr.columns)):\n",
    "        b = corr.columns[j]\n",
    "        G_all.add_edge(a, b, weight=float(D.iloc[i, j])) # type: ignore\n",
    "\n",
    "T = nx.minimum_spanning_tree(G_all, weight='weight')\n",
    "print(f\"MST edges: {T.number_of_edges()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d769171",
   "metadata": {},
   "source": [
    "## 6) Visualize Graph by Community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf7351e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pos = nx.spring_layout(G, seed=1, weight='weight', k=None, iterations=100)\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "for i, com in enumerate(communities):\n",
    "    nx.draw_networkx_nodes(G, pos, nodelist=list(com), node_size=80, label=f\"C{i}\")\n",
    "nx.draw_networkx_edges(G, pos, alpha=0.15)\n",
    "plt.title(\"Correlation Graph â€” Communities\")\n",
    "plt.legend(markerscale=2, fontsize=8, ncol=2)\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e01f54",
   "metadata": {},
   "source": [
    "## 7) Lightweight GNN-Style Message Passing (NumPy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eadff01",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_node_features(rets: pd.DataFrame, window: int = 20) -> pd.DataFrame:\n",
    "    \"\"\"Simple statistical features per asset: mean, vol, skew (approx), kurtosis (approx).\"\"\"\n",
    "    X = pd.DataFrame(index=rets.columns, columns=['mu', 'vol', 'skew', 'kurt'], dtype=float)\n",
    "    R = rets.tail(window)\n",
    "    mu = R.mean().values\n",
    "    vol = R.std(ddof=1).values + 1e-8 # type: ignore\n",
    "    z = (R - mu) / vol\n",
    "    skew = (z**3).mean().values\n",
    "    kurt = (z**4).mean().values - 3.0\n",
    "    X.loc[:, 'mu'] = mu\n",
    "    X.loc[:, 'vol'] = vol\n",
    "    X.loc[:, 'skew'] = skew\n",
    "    X.loc[:, 'kurt'] = kurt\n",
    "    return X\n",
    "\n",
    "X0 = build_node_features(rets, window=60)\n",
    "X0.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a786ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def relu(x): return np.maximum(0, x)\n",
    "\n",
    "def message_passing(G: nx.Graph, X: pd.DataFrame, layers: int = 2, alpha_self: float = 0.5) -> pd.DataFrame:\n",
    "    \"\"\"Simple symmetric normalize + aggregate: H^{(l+1)} = ReLU(alpha*H + (1-alpha)*A_hat H W_l) with identity weights.\n",
    "\n",
    "    Here, we avoid learning and use identity 'weights' to produce smooth embeddings.\"\"\"\n",
    "    nodes = list(X.index)\n",
    "    idx = {n:i for i,n in enumerate(nodes)}\n",
    "    # Build normalized adjacency (A_hat)\n",
    "    A = np.zeros((len(nodes), len(nodes)), dtype=float)\n",
    "    for u, v, d in G.edges(data=True):\n",
    "        i, j = idx[u], idx[v]\n",
    "        w = abs(d.get('weight', 0.0))\n",
    "        A[i, j] = w\n",
    "        A[j, i] = w\n",
    "    # Symmetric normalization\n",
    "    deg = A.sum(axis=1)\n",
    "    deg[deg == 0] = 1.0\n",
    "    Dm12 = np.diag(1.0/np.sqrt(deg))\n",
    "    A_hat = Dm12 @ A @ Dm12\n",
    "    H = X.values.copy()\n",
    "    for _ in range(layers):\n",
    "        H = relu(alpha_self*H + (1.0 - alpha_self)*(A_hat @ H))\n",
    "    return pd.DataFrame(H, index=nodes, columns=[f\"emb{i}\" for i in range(H.shape[1])])\n",
    "\n",
    "H = message_passing(G, X0, layers=2, alpha_self=0.6)\n",
    "H.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd13638",
   "metadata": {},
   "source": [
    "## 8) 2D Projection (PCA) and Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e48876",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pca_2d(X: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    Xc = X - X.mean(axis=0, keepdims=True)\n",
    "    C = np.cov(Xc, rowvar=False)\n",
    "    vals, vecs = np.linalg.eigh(C)\n",
    "    order = np.argsort(vals)[::-1]\n",
    "    vecs = vecs[:, order[:2]]\n",
    "    proj = Xc @ vecs\n",
    "    return proj, vals[order], vecs\n",
    "\n",
    "proj, evals, evecs = pca_2d(H.values)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "for i, n in enumerate(H.index):\n",
    "    c = comm_map.get(n, -1)\n",
    "    plt.scatter(proj[i,0], proj[i,1], s=30, alpha=0.8)\n",
    "plt.title(\"Node Embeddings (PCA of Message-Passed Features)\")\n",
    "plt.xlabel(\"PC1\"); plt.ylabel(\"PC2\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c7b89f",
   "metadata": {},
   "source": [
    "## 9) Link Similarity vs Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca946c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cosine similarity in embedding space vs. original correlation\n",
    "from numpy.linalg import norm\n",
    "\n",
    "tickers = list(H.index)\n",
    "emb = H.values\n",
    "sim = np.zeros_like(corr.values)\n",
    "for i in range(len(tickers)):\n",
    "    for j in range(len(tickers)):\n",
    "        a, b = emb[i], emb[j]\n",
    "        denom = (norm(a)*norm(b) + 1e-12)\n",
    "        sim[i,j] = float(np.dot(a,b)/denom)\n",
    "\n",
    "# Compare distributions of sim for edges vs. non-edges\n",
    "edges = [(i,j) for i in range(len(tickers)) for j in range(i+1,len(tickers)) if G.has_edge(tickers[i], tickers[j])]\n",
    "non_edges = [(i,j) for i in range(len(tickers)) for j in range(i+1,len(tickers)) if not G.has_edge(tickers[i], tickers[j])]\n",
    "\n",
    "def sample_pairs(pairs, k=5000, seed=0):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    if len(pairs) <= k:\n",
    "        return pairs\n",
    "    idx = rng.choice(len(pairs), size=k, replace=False)\n",
    "    return [pairs[t] for t in idx]\n",
    "\n",
    "edges_s = sample_pairs(edges, k=4000)\n",
    "non_s   = sample_pairs(non_edges, k=4000)\n",
    "\n",
    "edge_vals = np.array([sim[i,j] for i,j in edges_s])\n",
    "non_vals  = np.array([sim[i,j] for i,j in non_s])\n",
    "\n",
    "plt.figure(figsize=(9,4))\n",
    "plt.hist(edge_vals, bins=40, alpha=0.6, label='Edges (|corr|>=thr)')\n",
    "plt.hist(non_vals, bins=40, alpha=0.6, label='Non-edges')\n",
    "plt.title(\"Embedding Cosine Similarity for Edges vs. Non-edges\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Mean cos-sim (edges):   \", edge_vals.mean())\n",
    "print(\"Mean cos-sim (non-edge):\", non_vals.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b56faf7",
   "metadata": {},
   "source": [
    "## 10) Rolling Correlation Stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5d9f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rolling_corr_stability(rets: pd.DataFrame, window: int = 60, step: int = 20, threshold: float = 0.35) -> pd.DataFrame:\n",
    "    \"\"\"Compute Jaccard similarity of edge sets across rolling windows.\"\"\"\n",
    "    idx = rets.index\n",
    "    out = []\n",
    "    for start in range(0, len(idx)-2*window, step):\n",
    "        A = rets.iloc[start:start+window]\n",
    "        B = rets.iloc[start+step:start+step+window]\n",
    "        GA = build_graph_from_corr(A.corr(), threshold=threshold)\n",
    "        GB = build_graph_from_corr(B.corr(), threshold=threshold)\n",
    "        EA = set(tuple(sorted(e)) for e in GA.edges())\n",
    "        EB = set(tuple(sorted(e)) for e in GB.edges())\n",
    "        inter = len(EA & EB); union = len(EA | EB)\n",
    "        j = inter/union if union > 0 else np.nan\n",
    "        out.append((rets.index[start+step], j))\n",
    "    return pd.Series(dict(out)).to_frame('jaccard')\n",
    "\n",
    "stab = rolling_corr_stability(rets, window=80, step=20, threshold=0.35)\n",
    "stab.plot(title='Rolling Edge-Set Stability (Jaccard)')\n",
    "plt.ylabel('Jaccard Similarity')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
