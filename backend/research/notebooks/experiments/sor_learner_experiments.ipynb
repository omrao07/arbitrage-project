{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dcb6d24",
   "metadata": {},
   "source": [
    "\n",
    "# ðŸ§ª Smart Order Router (SOR) â€” RL Experiments\n",
    "\n",
    "This notebook simulates a **multi-venue trading environment** and trains a simple **reinforcement learning agent** to decide which venue to route an order to.\n",
    "\n",
    "**Experiments**\n",
    "- Synthetic environment with spreads, latencies, liquidity\n",
    "- Baseline strategies: round-robin, VWAP\n",
    "- RL Agent (policy gradient) learns venue selection\n",
    "- Compare performance across metrics: cost, slippage, latency\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709709d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75c042f",
   "metadata": {},
   "source": [
    "## 1) Synthetic Multi-Venue Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72b529e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiVenueEnv:\n",
    "    def __init__(self, n_venues=3, base_price=100.0):\n",
    "        self.n_venues = n_venues\n",
    "        self.base_price = base_price\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.step_count = 0\n",
    "        return self._state()\n",
    "\n",
    "    def _state(self):\n",
    "        # Randomized liquidity, spreads, latency\n",
    "        spreads = np.random.uniform(0.01, 0.05, size=self.n_venues)\n",
    "        liquidity = np.random.uniform(50, 200, size=self.n_venues)\n",
    "        latency = np.random.uniform(1, 10, size=self.n_venues)\n",
    "        return np.concatenate([spreads, liquidity/200.0, latency/10.0])\n",
    "\n",
    "    def step(self, action):\n",
    "        self.step_count += 1\n",
    "        spreads = np.random.uniform(0.01, 0.05, size=self.n_venues)\n",
    "        liquidity = np.random.uniform(50, 200, size=self.n_venues)\n",
    "        latency = np.random.uniform(1, 10, size=self.n_venues)\n",
    "        # Execution cost ~ spread - liquidity impact + latency penalty\n",
    "        cost = spreads[action] + 0.01*(100/liquidity[action]) + 0.001*latency[action]\n",
    "        reward = -cost  # lower cost is better\n",
    "        state = np.concatenate([spreads, liquidity/200.0, latency/10.0])\n",
    "        done = self.step_count >= 50\n",
    "        return state, reward, done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a173c7e1",
   "metadata": {},
   "source": [
    "## 2) Baseline Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2441600",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def baseline_round_robin(env, episodes=20):\n",
    "    rewards = []\n",
    "    for ep in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total = 0\n",
    "        t = 0\n",
    "        while not done:\n",
    "            action = t % env.n_venues\n",
    "            state, r, done = env.step(action)\n",
    "            total += r\n",
    "            t += 1\n",
    "        rewards.append(total)\n",
    "    return np.mean(rewards)\n",
    "\n",
    "def baseline_random(env, episodes=20):\n",
    "    rewards = []\n",
    "    for ep in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total = 0\n",
    "        while not done:\n",
    "            action = np.random.randint(env.n_venues)\n",
    "            state, r, done = env.step(action)\n",
    "            total += r\n",
    "        rewards.append(total)\n",
    "    return np.mean(rewards)\n",
    "\n",
    "env = MultiVenueEnv(n_venues=3)\n",
    "print(\"Round-robin baseline:\", baseline_round_robin(env))\n",
    "print(\"Random baseline:\", baseline_random(env))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbc90f0",
   "metadata": {},
   "source": [
    "## 3) Policy Gradient Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9617a16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, state_dim, n_actions):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, n_actions),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "def select_action(model, state):\n",
    "    state_t = torch.tensor(state, dtype=torch.float32)\n",
    "    probs = model(state_t)\n",
    "    dist = torch.distributions.Categorical(probs)\n",
    "    action = dist.sample()\n",
    "    return action.item(), dist.log_prob(action)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfa7a47",
   "metadata": {},
   "source": [
    "## 4) Train Policy Gradient Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35813e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_pg(env, episodes=200, gamma=0.99, lr=1e-2):\n",
    "    state_dim = env.reset().shape[0]\n",
    "    model = PolicyNet(state_dim, env.n_venues)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    all_rewards, losses = [], []\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        state = env.reset()\n",
    "        log_probs, rewards = [], []\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, logp = select_action(model, state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            log_probs.append(logp)\n",
    "            rewards.append(reward)\n",
    "            state = next_state\n",
    "\n",
    "        # Discounted return\n",
    "        returns, G = [], 0\n",
    "        for r in reversed(rewards):\n",
    "            G = r + gamma*G\n",
    "            returns.insert(0, G)\n",
    "        returns = torch.tensor(returns, dtype=torch.float32)\n",
    "        returns = (returns - returns.mean())/(returns.std()+1e-8)\n",
    "\n",
    "        loss = -(torch.stack(log_probs) * returns).sum()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        all_rewards.append(np.sum(rewards))\n",
    "        losses.append(loss.item())\n",
    "        if (ep+1) % 20 == 0:\n",
    "            print(f\"Episode {ep+1}: avg_reward={np.mean(all_rewards[-20:]):.4f}\")\n",
    "    return model, all_rewards, losses\n",
    "\n",
    "model, rewards, losses = train_pg(env, episodes=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813363cb",
   "metadata": {},
   "source": [
    "## 5) Results & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1b0731",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(rewards, label=\"Episode reward\")\n",
    "plt.title(\"Policy Gradient Training â€” SOR Agent\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
