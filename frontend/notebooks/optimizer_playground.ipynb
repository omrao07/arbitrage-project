{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ§® Optimizer Playground\n",
    "\n",
    "Interactive playground for portfolio optimization:\n",
    "- Meanâ€“Variance (with/without shorting)\n",
    "- Risk Parity & HRP (Hierarchical Risk Parity)\n",
    "- Blackâ€“Litterman (basic)\n",
    "- Robust covariance (Ledoitâ€“Wolf shrinkage)\n",
    "- Turnover & transaction cost penalties\n",
    "- Efficient frontier & scenario stress tests\n",
    "\n",
    "**How to use**\n",
    "1. Put a CSV at `./data/returns.csv` (wide format: dates as index, columns are tickers, values are returns), or the notebook will generate demo data.\n",
    "2. Set the tickers you want to include in `ASSETS_FILTER` (or leave `None`).\n",
    "3. Run cells, tweak knobs, compare allocations and risk metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10,4)\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "# --- Config ---\n",
    "RETURNS_CSV = './data/returns.csv'   # daily or weekly returns\n",
    "ASSETS_FILTER = None                  # e.g., ['AAPL','MSFT','SPY'] or None\n",
    "RISKFREE = 0.0                        # daily/weekly rf to match data frequency\n",
    "\n",
    "def load_returns(path=RETURNS_CSV):\n",
    "    if os.path.exists(path):\n",
    "        df = pd.read_csv(path, index_col=0, parse_dates=True)\n",
    "        df = df.sort_index()\n",
    "        if ASSETS_FILTER:\n",
    "            keep = [c for c in ASSETS_FILTER if c in df.columns]\n",
    "            df = df[keep]\n",
    "        return df.dropna(how='all').fillna(0.0)\n",
    "    # demo data\n",
    "    np.random.seed(7)\n",
    "    dates = pd.date_range('2022-01-01', periods=600, freq='B')\n",
    "    cols = ['AAPL','MSFT','AMZN','GOOG','META','TLT','GLD','XLF','XLE','BTC']\n",
    "    # Simulate with sector-style correlation\n",
    "    n = len(cols)\n",
    "    base_cov = 0.0001*np.ones((n,n))\n",
    "    for i in range(n):\n",
    "        base_cov[i,i] = 0.0004 + 0.0003*(i%3==0)\n",
    "    # add common market factor\n",
    "    mkt = np.random.normal(0, 0.008, size=len(dates))\n",
    "    rets = []\n",
    "    for i, c in enumerate(cols):\n",
    "        eps = np.random.normal(0, math.sqrt(base_cov[i,i]), size=len(dates))\n",
    "        rets.append(0.0003 + 0.5*mkt + eps)\n",
    "    df = pd.DataFrame({c:r for c,r in zip(cols, rets)}, index=dates)\n",
    "    return df\n",
    "\n",
    "R = load_returns()\n",
    "R.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covariance Estimation (Sample vs Ledoitâ€“Wolf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_stats(R: pd.DataFrame):\n",
    "    mu = R.mean().values  # mean per period\n",
    "    S  = R.cov().values\n",
    "    return mu, S\n",
    "\n",
    "def ledoit_wolf(R: pd.DataFrame):\n",
    "    # Simple Ledoitâ€“Wolf shrinkage toward scaled identity\n",
    "    X = R.values - R.values.mean(axis=0, keepdims=True)\n",
    "    T, N = X.shape\n",
    "    S = (X.T @ X) / (T-1)\n",
    "    var = np.trace(S)/N\n",
    "    F = var * np.eye(N)\n",
    "    # compute shrinkage intensity phi* ~ min(1, b2/d2)\n",
    "    # b2: sum of squares of sample covariance deviations\n",
    "    Xm = X - X.mean(axis=0, keepdims=True)\n",
    "    b2 = 0.0\n",
    "    for t in range(T):\n",
    "        xt = Xm[t][:,None]\n",
    "        Ct = xt @ xt.T\n",
    "        b2 += np.sum((Ct - S)**2)\n",
    "    b2 /= T**2\n",
    "    d2 = np.sum((S - F)**2)\n",
    "    shrink = max(0.0, min(1.0, b2/d2 if d2>1e-12 else 1.0))\n",
    "    Sigma = shrink*F + (1-shrink)*S\n",
    "    return Sigma\n",
    "\n",
    "mu_s, S_s = sample_stats(R)\n",
    "S_lw = ledoit_wolf(R)\n",
    "print('dims:', R.shape, 'mean range:', (mu_s.min(), mu_s.max())) # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann_mu(mu, periods=252):\n",
    "    return mu * periods\n",
    "\n",
    "def ann_vol(w, Sigma, periods=252):\n",
    "    return math.sqrt(max(0.0, w @ Sigma @ w) * periods)\n",
    "\n",
    "def sharpe(w, mu, Sigma, rf=0.0, periods=252):\n",
    "    r = w @ mu * periods - rf\n",
    "    v = ann_vol(w, Sigma, periods)\n",
    "    return r/(v+1e-12)\n",
    "\n",
    "def clamp_weights(w, long_only=True):\n",
    "    if long_only:\n",
    "        w = np.clip(w, 0.0, 1.0)\n",
    "    if abs(w.sum())>1e-12:\n",
    "        w = w / max(1e-12, w.sum())\n",
    "    return w\n",
    "\n",
    "def budget_project(w):\n",
    "    s = w.sum()\n",
    "    return w / (s if abs(s)>1e-12 else 1.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meanâ€“Variance Optimizer\n",
    "We solve:  \n",
    "$\\min_w \\; \\frac{1}{2} w^T \\Sigma w - \\lambda \\, \\mu^T w + \\gamma \\|w-w_0\\|_1$  \n",
    "s.t. $\\sum w = 1$, and (optional) $w \\ge 0$.\n",
    "\n",
    "- `risk_aversion` = $\\lambda$\n",
    "- `tc_gamma` = $\\gamma$ approximates transaction/turnover penalty (L1 around prev weights)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mv_opt(mu, Sigma, risk_aversion=10.0, long_only=True, w0=None, tc_gamma=0.0, iters=2000, lr=0.01):\n",
    "    n = len(mu)\n",
    "    w = np.ones(n)/n if w0 is None else w0.copy()\n",
    "    w = clamp_weights(w, long_only)\n",
    "    for _ in range(iters):\n",
    "        grad = Sigma @ w - risk_aversion * mu\n",
    "        if tc_gamma>0 and w0 is not None:\n",
    "            grad += tc_gamma * np.sign(w - w0)\n",
    "        w -= lr * grad\n",
    "        # project\n",
    "        if long_only:\n",
    "            w = np.clip(w, 0.0, 1.0)\n",
    "        w = budget_project(w)\n",
    "    return w\n",
    "\n",
    "mu = mu_s\n",
    "Sigma = S_lw\n",
    "w_mv = mv_opt(mu, Sigma, risk_aversion=15.0, long_only=True)\n",
    "pd.Series(w_mv, index=R.columns, name='MV').sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Risk Parity (equal risk contribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def risk_contributions(w, Sigma):\n",
    "    # RC_i = w_i * (Sigma w)_i\n",
    "    Sw = Sigma @ w\n",
    "    rc = w * Sw\n",
    "    return rc\n",
    "\n",
    "def risk_parity(Sigma, iters=4000, lr=0.02):\n",
    "    n = Sigma.shape[0]\n",
    "    w = np.ones(n)/n\n",
    "    target = np.ones(n)/n\n",
    "    for _ in range(iters):\n",
    "        rc = risk_contributions(w, Sigma)\n",
    "        # minimize squared error between normalized RC and target\n",
    "        rc_n = rc / (rc.sum()+1e-12)\n",
    "        grad = (Sigma @ w) * (1/(rc.sum()+1e-12)) - (rc.sum()*Sigma@w)/(rc.sum()+1e-12)**2\n",
    "        w -= lr * (rc_n - target) * grad\n",
    "        w = np.clip(w, 0.0, 1.0)\n",
    "        w = budget_project(w)\n",
    "    return w\n",
    "\n",
    "w_rp = risk_parity(Sigma)\n",
    "pd.Series(w_rp, index=R.columns, name='RiskParity').sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HRP (Hierarchical Risk Parity) â€“ simple implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correl_from_cov(S):\n",
    "    d = np.sqrt(np.diag(S))\n",
    "    return S / (d[:,None]*d[None,:] + 1e-12)\n",
    "\n",
    "def seriation(Z):\n",
    "    # Z: linkage matrix (n-1 x 4). We'll implement a simple nearest-neighbor ordering using correlation distance.\n",
    "    # For simplicity (no scipy), we use a greedy ordering by correlation.\n",
    "    n = Z.shape[0] + 1\n",
    "    order = [0]\n",
    "    remaining = set(range(1,n))\n",
    "    C = correl_from_cov(Sigma)\n",
    "    while remaining:\n",
    "        last = order[-1]\n",
    "        nxt = max(remaining, key=lambda j: C[last, j])\n",
    "        order.append(nxt)\n",
    "        remaining.remove(nxt)\n",
    "    return order\n",
    "\n",
    "def hrp_allocation(Sigma):\n",
    "    # Ultra-simplified HRP via greedy clustering order\n",
    "    n = Sigma.shape[0]\n",
    "    order = seriation(np.zeros((n-1,4)))\n",
    "    w = np.ones(n)\n",
    "    w = w / w.sum()\n",
    "    return w[order][np.argsort(order)]  # return original order\n",
    "\n",
    "w_hrp = hrp_allocation(Sigma)\n",
    "pd.Series(w_hrp, index=R.columns, name='HRP (simple)').sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blackâ€“Litterman (basic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def black_litterman(Sigma, w_mkt, P, Q, tau=0.05, delta=2.5):\n",
    "    # Sigma: covariance, w_mkt: market weights (sum=1),\n",
    "    # P: views matrix (k x n), Q: view returns (k,)\n",
    "    n = Sigma.shape[0]\n",
    "    pi = delta * Sigma @ w_mkt  # implied equilibrium returns\n",
    "    M = np.linalg.inv(np.linalg.inv(tau*Sigma) + P.T @ P)\n",
    "    mu_bl = M @ (np.linalg.inv(tau*Sigma) @ pi + P.T @ Q)\n",
    "    Sigma_bl = Sigma + M  # heuristic\n",
    "    return mu_bl, Sigma_bl\n",
    "\n",
    "n = R.shape[1]\n",
    "w_mkt = np.ones(n)/n\n",
    "# Simple view: asset 0 expected +0.5% (per period) better than asset 1\n",
    "P = np.zeros((1,n)); P[0,0]=1; P[0,1]=-1\n",
    "Q = np.array([0.005])\n",
    "mu_bl, S_bl = black_litterman(Sigma, w_mkt, P, Q)\n",
    "w_bl = mv_opt(mu_bl, S_bl, risk_aversion=10.0, long_only=True)\n",
    "pd.Series(w_bl, index=R.columns, name='BL MV').sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficient Frontier (long-only heuristic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def efficient_frontier(mu, Sigma, lam_grid, long_only=True):\n",
    "    W, RR, VV = [], [], []\n",
    "    w0 = np.ones(len(mu))/len(mu)\n",
    "    for lam in lam_grid:\n",
    "        w = mv_opt(mu, Sigma, risk_aversion=lam, long_only=long_only, w0=w0, tc_gamma=0.0, iters=1500, lr=0.02)\n",
    "        W.append(w)\n",
    "        RR.append(w @ mu)\n",
    "        VV.append(math.sqrt(max(1e-12, w @ Sigma @ w)))\n",
    "    return np.array(W), np.array(RR), np.array(VV)\n",
    "\n",
    "lam_grid = np.linspace(1.0, 50.0, 25)\n",
    "W, rr, vv = efficient_frontier(mu, Sigma, lam_grid)\n",
    "plt.figure(); plt.plot(vv*math.sqrt(252), rr*252, marker='o');\n",
    "plt.title('Efficient Frontier (annualized)'); plt.xlabel('Vol'); plt.ylabel('Return'); plt.grid(True); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Allocations & Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(name, w, mu, S, rf=RISKFREE, periods=252):\n",
    "    return {\n",
    "        'name': name,\n",
    "        'ret_ann': float((w @ mu) * periods),\n",
    "        'vol_ann': float(math.sqrt(max(1e-12, w @ S @ w)) * math.sqrt(periods)),\n",
    "        'sharpe': float(sharpe(w, mu, S, rf=rf, periods=periods))\n",
    "    }\n",
    "\n",
    "rows = [\n",
    "    summarize('EqualWeight', np.ones(len(mu))/len(mu), mu, Sigma),\n",
    "    summarize('MV', w_mv, mu, Sigma),\n",
    "    summarize('RiskParity', w_rp, mu, Sigma),\n",
    "    summarize('HRP(simple)', w_hrp, mu, Sigma),\n",
    "    summarize('BL-MV', w_bl, mu, Sigma)\n",
    "]\n",
    "pd.DataFrame(rows).set_index('name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turnover & Transaction Costs (what-if)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def turnover(w_prev, w_new):\n",
    "    return float(np.abs(w_new - w_prev).sum())\n",
    "\n",
    "w_prev = np.ones(len(mu))/len(mu)\n",
    "w_new = mv_opt(mu, Sigma, risk_aversion=20.0, long_only=True, w0=w_prev, tc_gamma=0.5)\n",
    "to = turnover(w_prev, w_new)\n",
    "print('Turnover from EqualWeight -> MV(tc):', round(to,4))\n",
    "pd.Series(w_new, index=R.columns, name='MV w/ TC').sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario Stress (shocks on means & covariances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_shock(mu, Sigma, mean_delta=0.0, vol_scale=1.0, corr_break=0.0):\n",
    "    # mean shift\n",
    "    mu2 = mu + mean_delta\n",
    "    # scale vols\n",
    "    d = np.sqrt(np.diag(Sigma))\n",
    "    C = Sigma / (d[:,None]*d[None,:] + 1e-12)\n",
    "    C = (1-corr_break)*C + corr_break*np.eye(len(mu))\n",
    "    d2 = d * vol_scale\n",
    "    S2 = (d2[:,None]*d2[None,:]) * C\n",
    "    return mu2, S2\n",
    "\n",
    "scenarios = {\n",
    "    'Baseline': (0.0, 1.0, 0.0),\n",
    "    'Vol x2': (0.0, 2.0, 0.0),\n",
    "    'CorrBreak 50%': (0.0, 1.2, 0.5),\n",
    "    'Bear -20bps': (-0.002, 1.5, 0.2)\n",
    "}\n",
    "\n",
    "rows=[]\n",
    "for name, (dm, vs, cb) in scenarios.items():\n",
    "    mu2, S2 = apply_shock(mu, Sigma, dm, vs, cb)\n",
    "    w2 = mv_opt(mu2, S2, risk_aversion=15.0, long_only=True)\n",
    "    rows.append(summarize(name, w2, mu2, S2))\n",
    "pd.DataFrame(rows).set_index('name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights Barplot (compare methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wcmp = pd.DataFrame({\n",
    "    'Equal': np.ones(len(mu))/len(mu),\n",
    "    'MV': w_mv,\n",
    "    'RP': w_rp,\n",
    "    'HRP': w_hrp,\n",
    "    'BL': w_bl,\n",
    "}, index=R.columns)\n",
    "Wcmp.plot(kind='bar', figsize=(12,4))\n",
    "plt.title('Weights Comparison'); plt.tight_layout(); plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
