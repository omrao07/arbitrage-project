{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "109d364c",
   "metadata": {},
   "source": [
    "\n",
    "# ES & VaR Scenario Analysis\n",
    "**Created:** 2025-08-31 20:23:13\n",
    "\n",
    "This notebook provides a complete, modular framework to compute **Value at Risk (VaR)** and **Expected Shortfall (ES/CVaR)** via multiple methods and run **stress/scenario** analyses. It also includes **backtesting** utilities (Kupiec POF test) and clean visualizations.\n",
    "\n",
    "### What you can do here\n",
    "- Load prices/returns from CSV or quickly **simulate** a synthetic asset path\n",
    "- Compute **Historical**, **Gaussian (Parametric)**, **Cornishâ€“Fisher**, and **Monte Carlo** VaR & ES\n",
    "- Run **multi-day horizons** and **shock** scenarios (volatility scaling, distribution tails)\n",
    "- **Backtest** VaR with the Kupiec Proportion of Failures test\n",
    "- Plot loss distributions and rolling VaR overlays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816b8b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== Setup ====\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import norm, t, skew, kurtosis\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.options.display.float_format = '{:,.6f}'.format\n",
    "\n",
    "# For reproducibility in examples\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87065ddb",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Data: Load or Simulate\n",
    "You can either:\n",
    "- **Load a CSV** with a price series (set `csv_path` and `price_col`), or\n",
    "- **Simulate** a synthetic Geometric Brownian Motion (GBM) series.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88378dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_returns_from_csv(csv_path: str, price_col: str = \"Close\", date_col: str = None):\n",
    "    \"\"\"\n",
    "    Load a price series from CSV and convert to daily log returns.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    if date_col and date_col in df.columns:\n",
    "        df[date_col] = pd.to_datetime(df[date_col])\n",
    "        df = df.sort_values(date_col).set_index(date_col)\n",
    "    else:\n",
    "        df = df.sort_index()\n",
    "    if price_col not in df.columns:\n",
    "        raise ValueError(f\"Column '{price_col}' not in CSV. Available: {list(df.columns)}\")\n",
    "    prices = df[price_col].astype(float).dropna()\n",
    "    rets = np.log(prices).diff().dropna()\n",
    "    rets.name = \"log_return\"\n",
    "    return rets\n",
    "\n",
    "def simulate_gbm_returns(n_days: int = 1500, mu: float = 0.08, sigma: float = 0.20, dt: float = 1/252):\n",
    "    \"\"\"\n",
    "    Simulate GBM prices and return daily log returns.\n",
    "    \"\"\"\n",
    "    eps = np.random.normal(0, 1, size=n_days)\n",
    "    log_rets = (mu - 0.5 * sigma**2) * dt + sigma * np.sqrt(dt) * eps\n",
    "    return pd.Series(log_rets, name=\"log_return\")\n",
    "\n",
    "# Example toggle: set to a file path to use your own data\n",
    "csv_path = None  # e.g., \"/mnt/data/my_prices.csv\"\n",
    "price_col = \"Close\"\n",
    "date_col = None\n",
    "\n",
    "if csv_path:\n",
    "    returns = load_returns_from_csv(csv_path, price_col=price_col, date_col=date_col)\n",
    "else:\n",
    "    returns = simulate_gbm_returns(n_days=2000, mu=0.07, sigma=0.25)\n",
    "\n",
    "returns = returns.dropna().astype(float)\n",
    "returns.tail()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ec5c59",
   "metadata": {},
   "source": [
    "\n",
    "## 2. VaR & ES Utilities\n",
    "We provide implementations for multiple approaches. By convention here:\n",
    "- **Losses** are negative returns.\n",
    "- VaR is quoted as a **positive** number (magnitude of the quantile loss).\n",
    "- ES is the **average loss beyond** the VaR threshold.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489fe630",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def historical_var_es(returns: pd.Series, alpha: float = 0.99):\n",
    "    \"\"\"\n",
    "    Historical VaR/ES using empirical quantiles of the loss distribution.\n",
    "    Returns VaR, ES (both positive magnitudes).\n",
    "    \"\"\"\n",
    "    losses = -returns.dropna().values\n",
    "    q = np.quantile(losses, alpha)\n",
    "    es = losses[losses >= q].mean() if np.any(losses >= q) else q\n",
    "    return q, es\n",
    "\n",
    "def gaussian_var_es(returns: pd.Series, alpha: float = 0.99):\n",
    "    \"\"\"\n",
    "    Parametric (Gaussian) VaR/ES using sample mean & std of returns.\n",
    "    \"\"\"\n",
    "    mu = returns.mean()\n",
    "    sd = returns.std(ddof=1)\n",
    "    z = norm.ppf(alpha)\n",
    "    var = -(mu - z * sd)  # VaR as positive magnitude of loss\n",
    "    es = -(mu - sd * norm.pdf(z) / (1 - alpha))\n",
    "    return float(var), float(es)\n",
    "\n",
    "def cornish_fisher_var_es(returns: pd.Series, alpha: float = 0.99):\n",
    "    \"\"\"\n",
    "    Cornish-Fisher adjusted VaR/ES accounting for skewness & excess kurtosis.\n",
    "    ES is approximated by numerical tail average using CF-perturbed quantiles.\n",
    "    \"\"\"\n",
    "    x = returns.dropna().values\n",
    "    mu, sd = x.mean(), x.std(ddof=1)\n",
    "    s, k = skew(x, bias=False), kurtosis(x, fisher=True, bias=False)  # excess kurtosis\n",
    "\n",
    "    z = norm.ppf(alpha)\n",
    "    z_cf = (z\n",
    "            + (1/6)*(z**2 - 1)*s\n",
    "            + (1/24)*(z**3 - 3*z)*k\n",
    "            - (1/36)*(2*z**3 - 5*z)*(s**2))\n",
    "    var = -(mu - z_cf * sd)\n",
    "\n",
    "    zs = np.linspace(norm.ppf(alpha), norm.ppf(0.999999), 2000)\n",
    "    zc = (zs\n",
    "          + (1/6)*(zs**2 - 1)*s\n",
    "          + (1/24)*(zs**3 - 3*zs)*k\n",
    "          - (1/36)*(2*zs**3 - 5*zs)*(s**2))\n",
    "    tail_losses = -(mu - zc * sd)\n",
    "    es = tail_losses.mean()\n",
    "    return float(var), float(es)\n",
    "\n",
    "def monte_carlo_var_es(returns: pd.Series, alpha: float = 0.99, n_sims: int = 200000, dist: str = \"normal\", nu: int = 5):\n",
    "    \"\"\"\n",
    "    Monte Carlo VaR/ES by simulating returns from a chosen distribution.\n",
    "    dist: \"normal\" or \"student_t\" (heavy tails via df=nu).\n",
    "    \"\"\"\n",
    "    mu = returns.mean()\n",
    "    sd = returns.std(ddof=1)\n",
    "\n",
    "    if dist == \"normal\":\n",
    "        sims = np.random.normal(mu, sd, size=n_sims)\n",
    "    elif dist == \"student_t\":\n",
    "        sims = mu + sd * (t.rvs(df=nu, size=n_sims) / np.sqrt(nu/(nu-2))) if nu > 2 else mu + sd * t.rvs(df=nu, size=n_sims)\n",
    "    else:\n",
    "        raise ValueError(\"dist must be 'normal' or 'student_t'\")\n",
    "\n",
    "    losses = -sims\n",
    "    q = np.quantile(losses, alpha)\n",
    "    es = losses[losses >= q].mean()\n",
    "    return float(q), float(es)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbadb76",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Horizons & Portfolio Scaling\n",
    "We include simple scaling for multi-day horizons and notional portfolio sizes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccbba92",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scale_to_horizon(var: float, es: float, horizon_days: int = 1, method: str = \"sqrt_time\"):\n",
    "    \"\"\"\n",
    "    Scale daily VaR/ES to multi-day horizon.\n",
    "    method=\"sqrt_time\": multiply by sqrt(horizon_days).\n",
    "    \"\"\"\n",
    "    if horizon_days <= 0:\n",
    "        raise ValueError(\"horizon_days must be >= 1\")\n",
    "    if method == \"sqrt_time\":\n",
    "        f = np.sqrt(horizon_days)\n",
    "        return var * f, es * f\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported scaling method\")\n",
    "\n",
    "def scale_to_notional(var: float, es: float, notional: float = 1_000_000):\n",
    "    \"\"\"\n",
    "    Convert return-based VaR/ES magnitudes to currency using notional exposure.\n",
    "    \"\"\"\n",
    "    return var * notional, es * notional\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ceead3",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Scenarios & Stress Testing\n",
    "Create bespoke shocks (e.g., volatility up, mean down), simulate from heavy-tailed distributions, or apply single-day crash scenarios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5240fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scenario_var_es(returns: pd.Series, alpha: float = 0.99, vol_scale: float = 1.0, mean_shift: float = 0.0,\n",
    "                    method: str = \"monte_carlo\", dist: str = \"student_t\", nu: int = 5, n_sims: int = 200000):\n",
    "    \"\"\"\n",
    "    Scenario engine: apply a mean/vol shock, then compute VaR/ES with chosen method.\n",
    "    \"\"\"\n",
    "    x = returns.dropna().values\n",
    "    mu = x.mean() + mean_shift\n",
    "    sd = x.std(ddof=1) * vol_scale\n",
    "    tmp = pd.Series(np.random.normal(mu, sd, size=len(x)))\n",
    "\n",
    "    if method == \"historical\":\n",
    "        return historical_var_es(tmp, alpha)\n",
    "    elif method == \"gaussian\":\n",
    "        return gaussian_var_es(tmp, alpha)\n",
    "    elif method == \"cornish_fisher\":\n",
    "        return cornish_fisher_var_es(tmp, alpha)\n",
    "    elif method == \"monte_carlo\":\n",
    "        series = pd.Series(np.random.normal(mu, sd, size=len(x)))\n",
    "        return monte_carlo_var_es(series, alpha=alpha, n_sims=n_sims, dist=dist, nu=nu)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown method\")\n",
    "\n",
    "def one_day_crash_var_es(returns: pd.Series, alpha: float = 0.99, crash_return: float = -0.10):\n",
    "    \"\"\"\n",
    "    Inject a single crash observation into the historical sample and recompute historical VaR/ES.\n",
    "    \"\"\"\n",
    "    augmented = pd.concat([returns.dropna(), pd.Series([crash_return])], ignore_index=True)\n",
    "    return historical_var_es(augmented, alpha)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d3cf80",
   "metadata": {},
   "source": [
    "\n",
    "## 5. VaR Backtesting (Kupiec POF)\n",
    "A simple **Proportion of Failures** test: compares observed exceedances vs. expected under the chosen confidence level.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56e3a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rolling_var(returns: pd.Series, alpha: float = 0.99, window: int = 250, method: str = \"historical\"):\n",
    "    \"\"\"\n",
    "    Compute rolling daily VaR using specified method.\n",
    "    Returns a Pandas Series aligned with input returns.\n",
    "    \"\"\"\n",
    "    vals = []\n",
    "    r = returns.dropna()\n",
    "    for i in range(len(r)):\n",
    "        if i < window:\n",
    "            vals.append(np.nan)\n",
    "            continue\n",
    "        sample = r.iloc[i-window:i]\n",
    "        if method == \"historical\":\n",
    "            var, _ = historical_var_es(sample, alpha)\n",
    "        elif method == \"gaussian\":\n",
    "            var, _ = gaussian_var_es(sample, alpha)\n",
    "        else:\n",
    "            var, _ = historical_var_es(sample, alpha)\n",
    "        vals.append(var)\n",
    "    return pd.Series(vals, index=r.index, name=f\"VaR_{method}_{int(alpha*100)}\")\n",
    "\n",
    "def kupiec_pof_test(returns: pd.Series, var_series: pd.Series, alpha: float = 0.99):\n",
    "    \"\"\"\n",
    "    Kupiec Proportion of Failures (POF) test.\n",
    "    Returns (LR, p_value, n_exceed, expected_exceed).\n",
    "    \"\"\"\n",
    "    aligned = pd.concat([returns, var_series], axis=1).dropna()\n",
    "    losses = -aligned.iloc[:,0].values\n",
    "    var_vals = aligned.iloc[:,1].values\n",
    "    exceed = (losses > var_vals).astype(int)\n",
    "    n = len(exceed)\n",
    "    x = exceed.sum()\n",
    "    pi = 1 - alpha\n",
    "    if x == 0 or x == n:\n",
    "        lr = 0 if x == 0 else np.inf\n",
    "        p = 1.0 if x == 0 else 0.0\n",
    "    else:\n",
    "        num = ((1 - pi)**(n - x)) * (pi**x)\n",
    "        den = ((1 - x/n)**(n - x)) * ((x/n)**x)\n",
    "        lr = -2 * np.log(num / den)\n",
    "        from scipy.stats import chi2\n",
    "        p = 1 - chi2.cdf(lr, df=1)\n",
    "    return float(lr), float(p), int(x), float(n * (1 - alpha))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9624eb0e",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Visualizations\n",
    "Charts are kept simple and clean (one chart per cell).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75cbef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_loss_histogram_with_var(returns: pd.Series, var_val: float, bins: int = 60, title: str = \"Loss Distribution & VaR\"):\n",
    "    losses = -returns.dropna().values\n",
    "    plt.figure(figsize=(9,5))\n",
    "    plt.hist(losses, bins=bins, alpha=0.7)\n",
    "    plt.axvline(var_val, linestyle=\"--\", linewidth=2)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Loss\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_rolling_var(returns: pd.Series, var_series: pd.Series, title: str = \"Rolling VaR vs PnL\"):\n",
    "    aligned = pd.concat([returns, var_series], axis=1).dropna()\n",
    "    plt.figure(figsize=(11,5))\n",
    "    plt.plot(aligned.index, aligned.iloc[:,0].cumsum(), linewidth=1.2, label=\"Cumulative PnL\")\n",
    "    plt.plot(aligned.index, -aligned.iloc[:,1], linewidth=1.2, label=\"-(VaR)\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Date Index\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d585df8a",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Example: Quick Run\n",
    "Below we compute VaR/ES by multiple methods on the current `returns` series and show a histogram.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfe4b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "alpha = 0.99\n",
    "\n",
    "h_var, h_es = historical_var_es(returns, alpha)\n",
    "g_var, g_es = gaussian_var_es(returns, alpha)\n",
    "cf_var, cf_es = cornish_fisher_var_es(returns, alpha)\n",
    "mc_var, mc_es = monte_carlo_var_es(returns, alpha, n_sims=200000, dist=\"student_t\", nu=5)\n",
    "\n",
    "print(\"=== Daily VaR/ES (return space, positive magnitudes) ===\")\n",
    "print(f\"Historical    VaR: {h_var:,.6f} | ES: {h_es:,.6f}\")\n",
    "print(f\"Gaussian      VaR: {g_var:,.6f} | ES: {g_es:,.6f}\")\n",
    "print(f\"CornishFisher VaR: {cf_var:,.6f} | ES: {cf_es:,.6f}\")\n",
    "print(f\"MonteCarlo t5 VaR: {mc_var:,.6f} | ES: {mc_es:,.6f}\")\n",
    "\n",
    "# Scale to 10-day horizon and 1m notional\n",
    "var_10d, es_10d = scale_to_horizon(h_var, h_es, horizon_days=10, method=\"sqrt_time\")\n",
    "var_cash, es_cash = scale_to_notional(var_10d, es_10d, notional=1_000_000)\n",
    "print(\"\\n=== 10-Day Historical (sqrt-time) in currency for 1,000,000 notional ===\")\n",
    "print(f\"VaR: {var_cash:,.2f} | ES: {es_cash:,.2f}\")\n",
    "\n",
    "plot_loss_histogram_with_var(returns, h_var, bins=60, title=f\"Loss Dist. & Historical VaR ({int(alpha*100)}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2838c8d",
   "metadata": {},
   "source": [
    "\n",
    "## 8. Scenario & Backtest Example\n",
    "- Apply a **volatility Ã—2** stress scenario (Monte Carlo, Studentâ€‘t).\n",
    "- Compute a **rolling historical VaR** and run **Kupiec POF**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44babb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Scenario: vol up 2x, mean -0.5 * original mean\n",
    "s_var, s_es = scenario_var_es(returns, alpha=alpha, vol_scale=2.0, mean_shift=-0.5*returns.mean(),\n",
    "                              method=\"monte_carlo\", dist=\"student_t\", nu=5, n_sims=150000)\n",
    "print(f\"Scenario (volx2, mean down): VaR {s_var:,.6f} | ES {s_es:,.6f}\")\n",
    "\n",
    "# One-day -10% crash injection (historical)\n",
    "c_var, c_es = one_day_crash_var_es(returns, alpha=alpha, crash_return=-0.10)\n",
    "print(f\"One-Day Crash (-10%) Historical: VaR {c_var:,.6f} | ES {c_es:,.6f}\")\n",
    "\n",
    "# Backtest\n",
    "roll = rolling_var(returns, alpha=alpha, window=250, method=\"historical\")\n",
    "lr, p, x, exp_x = kupiec_pof_test(returns, roll, alpha=alpha)\n",
    "print(f\"Kupiec POF: LR={lr:,.3f}, p={p:,.4f}, exceed={x}, expected={exp_x:,.2f}\")\n",
    "\n",
    "plot_rolling_var(returns, roll, title=f\"Rolling Historical VaR ({int(alpha*100)}%) vs CumPnL\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b9128f",
   "metadata": {},
   "source": [
    "\n",
    "## 9. Using with Your Portfolio\n",
    "- Replace the synthetic data by setting `csv_path` to your own file in the **Data** cell.\n",
    "- If you have **multiple assets**, compute portfolio returns with weights:\n",
    "\n",
    "```python\n",
    "# df_prices: DataFrame of prices (columns = tickers)\n",
    "log_returns = np.log(df_prices).diff().dropna()\n",
    "weights = np.array([0.4, 0.3, 0.3])  # example\n",
    "port_rets = (log_returns @ weights).rename(\"log_return\")\n",
    "```\n",
    "- Then run the same VaR/ES functions on `port_rets`.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
